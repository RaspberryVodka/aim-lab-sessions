{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86a684c",
   "metadata": {},
   "source": [
    "# Creating an image classifier\n",
    "\n",
    "A classic machine learning task is to train a model to classify images. Over the next few weeks, you will go through the steps in the machine learning pipeline to Although this is a simple task, it will take you through all the basic steps that are needed to build more complex models:\n",
    "1. Data handling\n",
    "2. Using Pytorch (a machine learning framework) to define models\n",
    "3. Create a training loop\n",
    "4. Metric logging\n",
    "\n",
    "Even relatively simple models can achieve very good results, as we will see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d98c9c",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "Pytorch is a framework that provides many convenience functions and classes that will help us build our machine learning network much faster than we would be able to from scratch using pure Python. It takes care of abstracting away the various components of your neural network (called `Module`s) and making sure that things like forward and backwards passes, gradient calculations, and weight updates are handled for you. \n",
    "\n",
    "## Subclassing the Dataset class\n",
    "Dataset is an abstract class provided by Pytorch, which means it defines a specification of what its subclasses must look like via function signatures. However, these functions are not actually implemented in the source code of the Dataset class. It's the programmer's job (yours!) to make sure the methods laid out in the signatures are implemented. Let's import these modules now, along with some other useful packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c598070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e380a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our dataset - it is a map-style dataset, which means we have to know its size at initialisation and  \n",
    "# must be able to access data points at arbitrary locations. This is why the following methods must be implemented:\n",
    "# __len__ and __getitem__ \n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, filepath: str): \n",
    "        super().__init__()\n",
    "        self.dataframe = pd.read_csv(filepath) # Load data from CSV filepath defined earlier into a Pandas dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) # Return size of our dataframe\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.dataframe.iloc[i] # Return the `i`th item in our dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfbecc",
   "metadata": {},
   "source": [
    "## The collate function\n",
    "The collate function is used to tell the Pytorch DataLoader how to handle datapoints from the MNISTDataset we defined earlier and pack them into a batch. By default (i.e. no specific collate_fn is passed), the DataLoader would simply add the dataset items to an array and ensure that the array is of a certain size (the batch size). This would normally not be a problem if we were working with text data that is of a fixed length.  \n",
    "\n",
    "However, in our case, we are working with image data, and our dataset (which is essentially just a Pandas DataFrame) does not actually contain the images themselves, but filepaths to them, along with labels. For this reason, we must define a custom collate function that reads these images and their labels into memory, and returns them side-by-side so we can use them in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb45b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860f2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \n",
    "    def load_image_tensor(filepath):\n",
    "        # This funtion is only visible inside custom_collate_fn and does the work of loading a single image into\n",
    "        # a Pytorch Tensor\n",
    "        img = Image.open(filepath)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.PILToTensor()\n",
    "        ])\n",
    "        img_tensor = transform(img)\n",
    "        return img_tensor\n",
    "\n",
    "    image_batch_tensor = torch.FloatTensor(len(batch), 28, 28) # We define a tensor of the same size as our image batch to store loaded images into\n",
    "    image_tensors = []\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        image_tensor = load_image_tensor(f\"{DATASET_PREFIX}/{item[0]}\") # load a single image\n",
    "        image_tensors.append(image_tensor) # put image into a list \n",
    "        labels.append(item[1]) # put the same image's label into another list\n",
    "\n",
    "\n",
    "    torch.cat(image_tensors, out=image_batch_tensor) # torch.cat simply concatenates a list of individual tensors (image_tensors) into a single Pytorch tensor (image_batch_tensor)\n",
    "    label_batch_tensor = torch.FloatTensor(labels).type(torch.int64) # use the label list to create a torch tensor of ints\n",
    "    return (image_batch_tensor, label_batch_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_path, batch_sz=100, train_val_test_split=[0.3, 0.2, 0.5]):\n",
    "    # This is a convenience funtion that returns dataset splits of train, val and test according to the fractions specified in the arguments\n",
    "    assert sum(train_val_test_split) == 1, \"Train, val and test fractions should sum to 1!\"  # Always a good idea to use static asserts when processing arguments that are passed in by a user!\n",
    "    train_dataset = MNISTDataset(data_path)  # Instantiating our previously defined dataset\n",
    "    \n",
    "    # This code generates the actual number of items that goes into each split using the user-supplied fractions\n",
    "    train_val_split = list(\n",
    "        map( # map applies a given function to each element of a list\n",
    "            lambda frac: round(frac * len(train_dataset)), # anonymous function that multiplies the fraction by total length of dataset and rounds to the nearest integer\n",
    "            train_val_test_split # the list to apply the function to\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # split dataset into train, val and test\n",
    "    train_split, val_split, test_split = random_split(train_dataset, train_val_split)\n",
    "    \n",
    "    # Use Pytorch DataLoader to load each split into memory. It's important to pass in our custom collate function, so it knows how to interpret the \n",
    "    # data and load it. num_workers tells the DataLoader how many CPU threads to use so that data can be loaded in parallel, which is faster\n",
    "    n_cpus = mp.cpu_count() # returns number of CPU cores on this machine\n",
    "    train_dl = DataLoader(train_split, \n",
    "                          batch_size=batch_sz, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=custom_collate_fn,\n",
    "                          num_workers=n_cpus)            \n",
    "    val_dl = DataLoader(val_split, \n",
    "                        batch_size=batch_sz, \n",
    "                        shuffle=True, \n",
    "                        collate_fn=custom_collate_fn,\n",
    "                        num_workers=n_cpus)\n",
    "    test_dl = DataLoader(test_split,\n",
    "                         batch_size=batch_sz,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=custom_collate_fn,\n",
    "                         num_workers=n_cpus)\n",
    "    return train_dl, val_dl, test_dl\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff50e7",
   "metadata": {},
   "source": [
    "## Visualising image data\n",
    "When working with image data, it can be helpful to define visualisation helper functions to make sure that the data visually \"looks right\". If you get an image at all, it is also a pretty good indication that you probably got all the dataloading code correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1815cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def image_grid(batch, ncols=4):\n",
    "    height, width = batch[0].shape\n",
    "    nrows = len(batch)//ncols # calculate the number of rows based on the number of columns needed by the user\n",
    "    \n",
    "    img_grid = (batch.reshape(nrows, ncols, height, width)\n",
    "              .swapaxes(1,2)\n",
    "              .reshape(height*nrows, width*ncols))\n",
    "    \n",
    "    return img_grid\n",
    "\n",
    "\n",
    "def show_batch(batch, title=\"Image batch\", cols=4):\n",
    "    N = len(batch)\n",
    "    if N > cols:\n",
    "        assert N % cols == 0, \"Number of cols must be a multiple of N\"\n",
    "    \n",
    "    result = image_grid(batch)\n",
    "    fig = plt.figure(figsize=(5., 5.))\n",
    "    plt.suptitle(f\"{title} [{int(N/cols)}x{cols}]\")\n",
    "    plt.imshow(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining global path variables\n",
    "MODEL_DIR = \"./saved_models\"\n",
    "DATASET_PREFIX = \"./data/MNIST\"\n",
    "DATA_PATH = f\"{DATASET_PREFIX}/raw/mnist_dataset.csv\"\n",
    "\n",
    "\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=16)\n",
    "\n",
    "train_images, _ = next(iter(train_dl))\n",
    "test_images, _ = next(iter(test_dl))\n",
    "\n",
    "show_batch(train_images, title=\"Train images\", cols=4)\n",
    "show_batch(test_images, title=\"Test images\", cols=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
