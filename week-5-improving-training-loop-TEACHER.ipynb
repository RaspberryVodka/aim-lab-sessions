{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2398eab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/kavi/Desktop/phd/teaching/aim-lab-sessions/vars/week_3.ipynb\n",
      "importing Jupyter notebook from /home/kavi/Desktop/phd/teaching/aim-lab-sessions/vars/week_4.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Get all the variables, classes and functions we defined in the previous lessons\n",
    "import import_ipynb\n",
    "from vars.week_3 import *\n",
    "from vars.week_4 import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bbfc9c",
   "metadata": {},
   "source": [
    "# 5. Improving the training loop\n",
    "## 5.1 Adaptive learning rate\n",
    "With the device selection out of the way, let's have a look at how we can improve the model's performance by modifiying one of its hyperparameters. You can think of hyperparameters as arguments to the model's training function. The reason they're not simply called parameters is because that word is reserved for the weights and biases within the network itself. In particular, we'll look at the model's learning rate.  \n",
    "\n",
    "From the lectures, you may remember that the learning rate of a model is a measure of how far the weights are allowed to 'step' in the direction of the gradient. If the LR is constant and large, (as was the case in our last example) the model takes large steps towards the optimal solution. This is not ideal: as the weights of the model start to converge on the solution, the large steps taken by the optimiser may undershoot it. A constant, small LR is not ideal either: the tiny steps taken by the optimiser mean that the model would take a very long time to converge, or learn. Therefore, we want a combination of these two: relatively large LR early on in the training (to quickly get in the neighbourhood of the optimal solution) and a progressively smaller LR as we get closer and closer to the solution (so that we don't undershoot it).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d758a",
   "metadata": {},
   "source": [
    "### 5.1.1 ExponentialLR\n",
    "In Pytorch, this is handled by a group of classes called learning rate schedulers. To demonstrate how they work, we will be using the `ExponentialLR` scheduler, which applies the following function to reduce the learning rate every epoch:    \n",
    "\n",
    "$l_{epoch} = g * l_{epoch - 1}$ \n",
    "\n",
    "where \n",
    "* $l_{epoch}$ is the new learning rate to be set for the next epoch,\n",
    "* $g$ is the hyperparameter gamma, which controls how quickly the learning rate decays, and\n",
    "* $l_{epoch - 1}$ is the learning rate of the current epoch\n",
    "\n",
    "_Note: The initial LR chosen for this exercise was actually relatively high and is only intended to exaggerate the difference an adaptive LR scheduler makes. Real applications initialise their LRs to something many orders of magnitude smaller_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6da7918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa03bf35b80>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "model = get_simple_linear_net()\n",
    "optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "gamma = 0.5 # gamma is the multiplier that gets applied every time the scheduler is stepped\n",
    "schd = ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "lrs = []  \n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    optimizer.step()\n",
    "    lrs.append(schd.get_last_lr())\n",
    "    schd.step()\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1b275",
   "metadata": {},
   "source": [
    "### 5.1.2 Modifying the training loop \n",
    "Let's apply what we've learned about adaptive LRs to our training loop. We update our `train_model` function to accept a new parameter `lr_scheduler`, which is an instance of the `ExponentialLR` class. You can of course use any LR scheduler in the `torch.optim.lr_scheduler` module, and some may be better suited to a particular task than others. `ExponentialLR` was chosen here for its simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4c8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_gpu_lr(device, model, epochs, train_dl, optimiser, lr_scheduler):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            \n",
    "            # Transferring image and label tensors to GPU #\n",
    "            image_batch = image_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            ###############################################\n",
    "            \n",
    "            output = model(image_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()  \n",
    "            \n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            correct += int(torch.eq(preds, label_batch).sum())\n",
    "            total += batch_sz\n",
    "            minibatch_accuracy = 100 * correct / total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            \n",
    "        lr_scheduler.step() # Call the LR scheduler every epoch so that it can update the learning rate used by the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3579496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[5/5], MiniBatch[390/394], Loss: 0.31133, Acc: 90.88141, LR: 0.00031\r"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "network = get_simple_linear_net()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network.to(device)\n",
    "train_model_gpu_lr(device, network, epochs, train_dl, optim, lr_sch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd68c4d",
   "metadata": {},
   "source": [
    "### 5.1.3 Debrief: Simple model with adaptive LR\n",
    "The model should now be performing at above 90% accuracy. Getting closer, but not quite there yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a245ee",
   "metadata": {},
   "source": [
    "## 5.2 The convolutional network\n",
    "Back to improving model performance - we're currently hovering around 90%, so let's modify our simple linear network so that the input is a convolutional layer. This is more suited to image data than a simple fully connected layer like we had in the previous few examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f79b7679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             416\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "           Flatten-4                 [-1, 3136]               0\n",
      "            Linear-5                  [-1, 128]         401,536\n",
      "              ReLU-6                  [-1, 128]               0\n",
      "            Linear-7                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 403,242\n",
      "Trainable params: 403,242\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.24\n",
      "Params size (MB): 1.54\n",
      "Estimated Total Size (MB): 1.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_simple_conv_net():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2), # no need for nn.Flatten: Conv2d expects a 2d array\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2, 2)), # output is out_channels * image_width/kernel_size * image_width/kernel_size\n",
    "        \n",
    "        nn.Flatten(), # output of MaxPool2d is multidimensional, so it needs to be flattened\n",
    "        nn.Linear(16 * 14 * 14, 128), # Linear layer like before\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "\n",
    "# Print the model's summary\n",
    "summary(get_simple_conv_net(), input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf187f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the model now expects a 3-d input (channels * width * height), we need to modify our training function:\n",
    "def train_model_gpu_lr_conv(device, model, epochs, train_dl, optimiser, lr_scheduler):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(device)\n",
    "            image_batch = image_batch.to(device).reshape(batch_sz, 1, 28, 28)  # 1 channel, 28 * 28 pixels\n",
    "            output = model(image_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()  \n",
    "            \n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            correct += int(torch.eq(preds, label_batch).sum())\n",
    "            total += batch_sz\n",
    "            minibatch_accuracy = 100 * correct / total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            \n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad94bf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[5/5], MiniBatch[390/394], Loss: 0.02174, Acc: 99.58333, LR: 0.00031\r"
     ]
    }
   ],
   "source": [
    "network = get_simple_conv_net()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
    "network = network.to(device)\n",
    "train_model_gpu_lr_conv(device, network, epochs, train_dl, optim, lr_sch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74f978",
   "metadata": {},
   "source": [
    "### 5.2.1 Debrief: Simple Conv net \n",
    "You should now see a significantly higher accuracy (around 98 - 99%). However, training accuracy is only half the story. We know now that the model can predict with very high accuracy what digit it's looking at as long as it's already seen it before. But what if it has to make a prediction on a number it's not seen before?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931037d5",
   "metadata": {},
   "source": [
    "## 5.3 The validation epoch\n",
    "This is the reason we split out dataset into train, validation and test splits earlier. At the end of every train epoch, we additionally run a validation epoch to assure ourselves that the model is actually learning a decent representation of the dataset in general, and not over-fitting on the training data. Once again, we must make the necessary changes to our training function (getting to be a bit of a mouthful now :D). The `train_dl` argument has been replaced by a dictionary that holds all the dataloaders associated with our dataset, so that we can reference them by name in the function body without passing a lot of arguments to it. It is generally a good idea to wrap closely related arguments in some sort of structure like a dictionary - leads to less cluttered function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5e21d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}\n",
    "\n",
    "def train_model_gpu_lr_conv_valid(device, model, epochs, dataloaders, optimiser, lr_scheduler):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):        \n",
    "        #######################TRAINING STEP###################################\n",
    "        model.train()  # set model to training mode \n",
    "        train_dl = dataloaders['train'] # select train dataloader\n",
    "        \n",
    "        total_steps_train = len(train_dl)\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(device)\n",
    "            image_batch = image_batch.to(device).reshape(batch_sz, 1, 28, 28) \n",
    "            output = model(image_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            preds_train = torch.argmax(output, dim=1)\n",
    "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
    "            total_train += batch_sz\n",
    "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
    "            \n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_train}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "        lr_scheduler.step()\n",
    "        ########################################################################\n",
    "        print(\"\") # Create newline between progress bars\n",
    "        #######################VALIDATION STEP##################################\n",
    "        model.eval()  # set model to evaluation mode. This is very important, we do not want to update model weights in eval mode\n",
    "        val_dl = dataloaders['val'] # select val dataloader\n",
    "        \n",
    "        total_steps_val = len(val_dl)\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        for batch_num, (image_batch, label_batch) in enumerate(val_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(device)\n",
    "            image_batch = image_batch.to(device).reshape(batch_sz, 1, 28, 28) \n",
    "            \n",
    "            with torch.no_grad(): # no_grad disables gradient calculations, which are not needed when evaluating the model. This speeds up the calculations\n",
    "                output = model(image_batch)\n",
    "                losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "                preds_val = torch.argmax(output, dim=1)\n",
    "                correct_val += int(torch.eq(preds_val, label_batch).sum())\n",
    "                total_val += batch_sz\n",
    "                minibatch_accuracy_val = 100 * correct_val / total_val\n",
    "                \n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "                if (batch_num + 1) % 5 == 0:\n",
    "                    print(\" \" * len(msg), end='\\r')\n",
    "                    msg = f'Eval epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_val}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy_val:.5f}'\n",
    "                    print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "        ########################################################################\n",
    "        print(\"\")  # Create newline between progress bars\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac6f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[1/5], MiniBatch[390/394], Loss: 0.13544, Acc: 82.63622, LR: 0.00500\n",
      "Eval epoch[1/5], MiniBatch[260/263], Loss: 0.11912, Acc: 94.11058              \n",
      "Train epoch[2/5], MiniBatch[390/394], Loss: 0.03916, Acc: 96.27404, LR: 0.00250\n",
      "Eval epoch[2/5], MiniBatch[260/263], Loss: 0.03310, Acc: 95.68510              \n",
      "Train epoch[3/5], MiniBatch[390/394], Loss: 0.09405, Acc: 98.02083, LR: 0.00125\n",
      "Eval epoch[3/5], MiniBatch[260/263], Loss: 0.01811, Acc: 96.11779              \n",
      "Train epoch[4/5], MiniBatch[390/394], Loss: 0.07040, Acc: 98.65385, LR: 0.00063\n",
      "Eval epoch[4/5], MiniBatch[260/263], Loss: 0.00440, Acc: 96.28606              \n",
      "Train epoch[5/5], MiniBatch[390/394], Loss: 0.02140, Acc: 98.88622, LR: 0.00031\n",
      "Eval epoch[5/5], MiniBatch[260/263], Loss: 0.13168, Acc: 96.17788              \n"
     ]
    }
   ],
   "source": [
    "network = get_simple_conv_net()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
    "network = network.to(device)\n",
    "train_model_gpu_lr_conv_valid(device, network, epochs, dataloaders, optim, lr_sch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b4c01",
   "metadata": {},
   "source": [
    "### 5.3.1 Debrief: Simple Conv net with validation \n",
    "You will notice that the evaluation accuracy lags behind the training accuracy ever so slightly. This is normal, and to be expected: it shows the model is not quite as good at predicting unseen numbers as it is at predicting ones it's seen before, although it's pretty close! The gap between the train and val metrics is called the generlisation gap and is a measure of how well the model performs on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
