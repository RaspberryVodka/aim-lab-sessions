{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2836d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/kavi/Desktop/phd/teaching/aim-lab-sessions/vars/week_3.ipynb\n",
      "importing Jupyter notebook from /home/kavi/Desktop/phd/teaching/aim-lab-sessions/vars/week_4.ipynb\n",
      "importing Jupyter notebook from /home/kavi/Desktop/phd/teaching/aim-lab-sessions/vars/week_5.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Get all the variables, classes and functions we defined in the previous lessons\n",
    "import import_ipynb\n",
    "from vars.week_3 import *\n",
    "from vars.week_4 import *\n",
    "from vars.week_5 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841b1eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 14, 14]           3,200\n",
      "         MaxPool2d-2             [-1, 64, 7, 7]               0\n",
      "       BatchNorm2d-3             [-1, 64, 7, 7]             128\n",
      "              ReLU-4             [-1, 64, 7, 7]               0\n",
      "            Conv2d-5             [-1, 64, 7, 7]          36,928\n",
      "       BatchNorm2d-6             [-1, 64, 7, 7]             128\n",
      "            Conv2d-7             [-1, 64, 7, 7]          36,928\n",
      "       BatchNorm2d-8             [-1, 64, 7, 7]             128\n",
      "          ResBlock-9             [-1, 64, 7, 7]               0\n",
      "           Conv2d-10             [-1, 64, 7, 7]          36,928\n",
      "      BatchNorm2d-11             [-1, 64, 7, 7]             128\n",
      "           Conv2d-12             [-1, 64, 7, 7]          36,928\n",
      "      BatchNorm2d-13             [-1, 64, 7, 7]             128\n",
      "         ResBlock-14             [-1, 64, 7, 7]               0\n",
      "           Conv2d-15            [-1, 128, 4, 4]           8,320\n",
      "      BatchNorm2d-16            [-1, 128, 4, 4]             256\n",
      "           Conv2d-17            [-1, 128, 4, 4]          73,856\n",
      "      BatchNorm2d-18            [-1, 128, 4, 4]             256\n",
      "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "         ResBlock-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "         ResBlock-26            [-1, 128, 4, 4]               0\n",
      "           Conv2d-27            [-1, 256, 2, 2]          33,024\n",
      "      BatchNorm2d-28            [-1, 256, 2, 2]             512\n",
      "           Conv2d-29            [-1, 256, 2, 2]         295,168\n",
      "      BatchNorm2d-30            [-1, 256, 2, 2]             512\n",
      "           Conv2d-31            [-1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-32            [-1, 256, 2, 2]             512\n",
      "         ResBlock-33            [-1, 256, 2, 2]               0\n",
      "           Conv2d-34            [-1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-35            [-1, 256, 2, 2]             512\n",
      "           Conv2d-36            [-1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-37            [-1, 256, 2, 2]             512\n",
      "         ResBlock-38            [-1, 256, 2, 2]               0\n",
      "           Conv2d-39            [-1, 512, 1, 1]         131,584\n",
      "      BatchNorm2d-40            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-41            [-1, 512, 1, 1]       1,180,160\n",
      "      BatchNorm2d-42            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-43            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-44            [-1, 512, 1, 1]           1,024\n",
      "         ResBlock-45            [-1, 512, 1, 1]               0\n",
      "           Conv2d-46            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-47            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-48            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-49            [-1, 512, 1, 1]           1,024\n",
      "         ResBlock-50            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-51            [-1, 512, 1, 1]               0\n",
      "          Flatten-52                  [-1, 512]               0\n",
      "           Linear-53                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,180,170\n",
      "Trainable params: 11,180,170\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.74\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 43.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# So far, we've looked at how to put together simple models using Pytorch's\n",
    "# nn.Sequential function. Remember, all it does is sequentially line up\n",
    "# the layers and directs the output of one into the input of the next. However,\n",
    "# there are cases where we don't want out data to flow in such a linear fashion.\n",
    "# A good example is a Residual Network or ResNet, which applies  \n",
    "# shortcuts between its layers, which cannot be done using a simple \n",
    "# nn.Sequential construction. In such cases, it becomes necessary to sub-class\n",
    "# the nn.Module class and create our own.\n",
    "\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample):\n",
    "        super().__init__()\n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        shortcut = self.shortcut(inp)\n",
    "        inp = nn.ReLU()(self.bn1(self.conv1(inp)))\n",
    "        inp = nn.ReLU()(self.bn2(self.conv2(inp)))\n",
    "        inp = inp + shortcut  # The magic bit that cannot be done with nn.Sequential!\n",
    "        return nn.ReLU()(inp)\n",
    "    \n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, resblock, outputs):\n",
    "        super().__init__()\n",
    "        # GET RID OF NN.SEQENTIAL TO SHOW THAT YOU DON'T HAVE TO RELY ON IT\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            resblock(64, 64, downsample=False),\n",
    "            resblock(64, 64, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            resblock(64, 128, downsample=True),\n",
    "            resblock(128, 128, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            resblock(128, 256, downsample=True),\n",
    "            resblock(256, 256, downsample=False)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            resblock(256, 512, downsample=True),\n",
    "            resblock(512, 512, downsample=False)\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flat = nn.Flatten() \n",
    "        self.fc = nn.Linear(512, outputs)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = self.layer0(inp)\n",
    "        inp = self.layer1(inp)\n",
    "        inp = self.layer2(inp)\n",
    "        inp = self.layer3(inp)\n",
    "        inp = self.layer4(inp)\n",
    "        inp = self.gap(inp)\n",
    "        inp = self.flat(inp)\n",
    "        inp = self.fc(inp)\n",
    "\n",
    "        return inp\n",
    "    \n",
    "\n",
    "# convenience function\n",
    "def get_resnet():\n",
    "    return ResNet(1, ResBlock, outputs=10)\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "summary(get_resnet(), input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dffdbdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[1/5], MiniBatch[390/394], Loss: 0.20282, Acc: 92.19551, LR: 0.00500\n",
      "Eval epoch[1/5], MiniBatch[260/263], Loss: 0.11775, Acc: 95.61298              \n",
      "Train epoch[2/5], MiniBatch[390/394], Loss: 0.03818, Acc: 98.29327, LR: 0.00250\n",
      "Eval epoch[2/5], MiniBatch[260/263], Loss: 0.02865, Acc: 97.92067              \n",
      "Train epoch[3/5], MiniBatch[390/394], Loss: 0.06368, Acc: 99.39904, LR: 0.00125\n",
      "Eval epoch[3/5], MiniBatch[260/263], Loss: 0.06906, Acc: 98.18510              \n",
      "Train epoch[4/5], MiniBatch[390/394], Loss: 0.00843, Acc: 99.78365, LR: 0.00063\n",
      "Eval epoch[4/5], MiniBatch[260/263], Loss: 0.17588, Acc: 98.12500              \n",
      "Train epoch[5/5], MiniBatch[390/394], Loss: 0.00477, Acc: 99.75962, LR: 0.00031\n",
      "Eval epoch[5/5], MiniBatch[260/263], Loss: 0.01058, Acc: 98.01683              \n"
     ]
    }
   ],
   "source": [
    "# We do not need to define a new train function as the only changes have been to the internal\n",
    "# structure of the model, not any of its inputs or outputs, so we can keep using\n",
    "# the old one - very handy!\n",
    "\n",
    "# loading data\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}\n",
    "\n",
    "# setting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "network = get_resnet()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
    "network = network.to(device)\n",
    "train_model_gpu_lr_conv_valid(device, network, epochs, dataloaders, optim, lr_sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2847fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training on a typical machine learning project is a time-consuming process. Sometimes, we may even want to \n",
    "# run the same model multiple times with different parameters to see which ones work the best (a process known as hyper-\n",
    "# parameter tuning). If a model is allowed to run for a long time despite not getting any better at learning,\n",
    "# that's wasted time and computing power. We use early stopping to tackle this problem. This is a simple algorithm \n",
    "# that just says whether or not the model should be stopped, based on its performance over time. \n",
    "\n",
    "# To illustrate this concept, we define a class EarlyStopper that performs this check. Its should_stop method must \n",
    "# be called every epoch with the current validation accuracy for that epoch\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, tolerance=0):\n",
    "        self.patience = patience           # How many epochs in a row the model is allowed to underperform    \n",
    "        self.tolerance = tolerance         # How much leeway the model has (i.e. how close it can get to underperforming before it is counted as such)\n",
    "        self.epoch_counter = 0             # Keeping track of how many epochs in a row were failed \n",
    "        self.max_validation_acc = np.NINF  # Keeping track of best metric so far\n",
    "\n",
    "    def should_stop(self, validation_acc):\n",
    "        if validation_acc > self.max_validation_acc:\n",
    "            self.max_validation_acc = validation_acc\n",
    "            self.epoch_counter = 0\n",
    "        elif validation_acc < (self.max_validation_acc - self.tolerance):\n",
    "            self.epoch_counter += 1\n",
    "            if self.epoch_counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "# We will also be using a logging tool called tensorboard to keep \n",
    "# track of our metrics and visualise the performance of our model as it goes\n",
    "# through its training cyle.\n",
    "\n",
    "# Finally, it is typical in machine learning pipelines to save the progress of a long-running \n",
    "# training session every so often. Since it is such a time consuming process, there are a \n",
    "# number of things outside the control of the programmer that might interrupt the training.\n",
    "# If such a thing were to happen in the middle of a training cyle, that could mean \n",
    "# days or weeks of training lost in an instant. It's STRONGLY recommended that you \n",
    "# implement some kind of checkpointing functionality in your ML training pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b412ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Saves a model to file, and names it after the current epoch\n",
    "def save_checkpoint(model, epoch, save_dir):\n",
    "    filename = f\"checkpoint_{epoch}.pth\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    torch.save(model, save_path)\n",
    "\n",
    "\n",
    "# Using all this information, we update our training function one last time.\n",
    "# For those of you keeping score, this training function now supports:\n",
    "# 1. Adaptive learning rates\n",
    "# 2. GPU training\n",
    "# 3. Training and validation cycles\n",
    "# 4. Early stopping\n",
    "# 5. Logging metrics \n",
    "# 6. Model checkpointing\n",
    "# To avoid ending up with a ridiculously long name for our training \n",
    "# function (train_model_gpu_lr_conv_valid_stopping_logging_checkpoint or some such),\n",
    "# we shall rename it one last time to something sensible. 3 additional arguments have been \n",
    "# added: the summary writer, the early stopper that we just wrote, and how often\n",
    "# we want to save checkpoints\n",
    "def train_model_final(device, model, epochs, dataloaders, optimiser, lr_scheduler, writer, early_stopper, checkpoint_frequency):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):        \n",
    "        #######################TRAINING STEP###################################\n",
    "        model.train()  # set model to training mode \n",
    "        train_dl = dataloaders['train'] # select train dataloader\n",
    "        \n",
    "        total_steps_train = len(train_dl)\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        loss_train = 0\n",
    "        \n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(device)\n",
    "            image_batch = image_batch.to(device).reshape(batch_sz, 1, 28, 28) \n",
    "            output = model(image_batch)\n",
    "            loss_train = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            preds_train = torch.argmax(output, dim=1)\n",
    "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
    "            total_train += batch_sz\n",
    "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
    "            \n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_train}], Loss: {loss_train.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "        lr_scheduler.step()\n",
    "        ########################################################################\n",
    "        print(\"\") # Create newline between progress bars\n",
    "        #######################VALIDATION STEP##################################\n",
    "        model.eval()  # set model to evaluation mode. This is very important, we do not want to update model weights in eval mode\n",
    "        val_dl = dataloaders['val'] # select val dataloader\n",
    "        \n",
    "        total_steps_val = len(val_dl)\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        loss_val = 0\n",
    "        \n",
    "        for batch_num, (image_batch, label_batch) in enumerate(val_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(device)\n",
    "            image_batch = image_batch.to(device).reshape(batch_sz, 1, 28, 28) \n",
    "            \n",
    "            with torch.no_grad(): # no_grad disables gradient calculations, which are not needed when evaluating the model. This speeds up the calculations\n",
    "                output = model(image_batch)\n",
    "                loss_val = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "                preds_val = torch.argmax(output, dim=1)\n",
    "                correct_val += int(torch.eq(preds_val, label_batch).sum())\n",
    "                total_val += batch_sz\n",
    "                minibatch_accuracy_val = 100 * correct_val / total_val\n",
    "                \n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "                if (batch_num + 1) % 5 == 0:\n",
    "                    print(\" \" * len(msg), end='\\r')\n",
    "                    msg = f'Eval epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_val}], Loss: {loss_val.item():.5f}, Acc: {minibatch_accuracy_val:.5f}'\n",
    "                    if early_stopper.epoch_counter > 0:\n",
    "                        msg += f\", Epochs without improvement: {early_stopper.epoch_counter}\"\n",
    "                    print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "        ########################################################################\n",
    "        print(\"\")  # Create newline between progress bars\n",
    "        \n",
    "        # Log loss and accuracy metrics using the writer so we can see them in Tensorboard \n",
    "        epoch_train_acc = 100 * correct_train / total_train\n",
    "        epoch_val_acc = 100 * correct_val / total_val\n",
    "        \n",
    "        writer.add_scalar(f'Loss/train', loss_train, epoch)\n",
    "        writer.add_scalar(f'Acc/train', epoch_train_acc, epoch)\n",
    "        writer.add_scalar(f'Loss/val', loss_val, epoch)\n",
    "        writer.add_scalar(f'Acc/val', epoch_val_acc, epoch)\n",
    "        \n",
    "        # Check whether we need to save the model to a checkpoint file\n",
    "        if (epoch + 1) % checkpoint_frequency == 0:\n",
    "            save_checkpoint(model, epoch + 1, \"./saved_models\")\n",
    "\n",
    "        # Check whether we should stop the training based on the validation accuracy\n",
    "        if early_stopper.should_stop(epoch_val_acc):\n",
    "            print(f\"\\nValidation accuracy has not improved for the last {early_stopper.epoch_counter} epochs, stopping training early at epoch {epoch + 1}/{epochs}\")\n",
    "            # if stopping, we also want to save the checkpoint so we don't lose anything between the last save\n",
    "            save_checkpoint(model, epoch + 1, \"./saved_models\")\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66091303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[1/15], MiniBatch[95/99], Loss: 0.13438, Acc: 87.69737, LR: 0.00500\n",
      "Eval epoch[1/15], MiniBatch[65/66], Loss: 0.18911, Acc: 94.91587              \n",
      "Train epoch[2/15], MiniBatch[95/99], Loss: 0.08409, Acc: 97.82895, LR: 0.00250\n",
      "Eval epoch[2/15], MiniBatch[65/66], Loss: 0.19604, Acc: 95.99760              \n",
      "Train epoch[3/15], MiniBatch[95/99], Loss: 0.05594, Acc: 99.07895, LR: 0.00125\n",
      "Eval epoch[3/15], MiniBatch[65/66], Loss: 0.06757, Acc: 96.09375              \n",
      "Train epoch[4/15], MiniBatch[95/99], Loss: 0.04338, Acc: 99.44079, LR: 0.00063\n",
      "Eval epoch[4/15], MiniBatch[65/66], Loss: 0.23184, Acc: 96.09375              \n",
      "Train epoch[5/15], MiniBatch[95/99], Loss: 0.02643, Acc: 99.62993, LR: 0.00031\n",
      "Eval epoch[5/15], MiniBatch[65/66], Loss: 0.12464, Acc: 96.12981              \n",
      "Train epoch[6/15], MiniBatch[95/99], Loss: 0.03355, Acc: 99.68750, LR: 0.00016\n",
      "Eval epoch[6/15], MiniBatch[65/66], Loss: 0.11778, Acc: 96.10577              \n",
      "Train epoch[7/15], MiniBatch[95/99], Loss: 0.06331, Acc: 99.58059, LR: 0.00008\n",
      "Eval epoch[7/15], MiniBatch[65/66], Loss: 0.14029, Acc: 96.21394, Epochs without improvement: 1\n",
      "Train epoch[8/15], MiniBatch[95/99], Loss: 0.03535, Acc: 99.72039, LR: 0.00004                 \n",
      "Eval epoch[8/15], MiniBatch[65/66], Loss: 0.14475, Acc: 96.15385              \n",
      "Train epoch[9/15], MiniBatch[95/99], Loss: 0.03357, Acc: 99.68750, LR: 0.00002\n",
      "Eval epoch[9/15], MiniBatch[65/66], Loss: 0.12133, Acc: 96.15385, Epochs without improvement: 1\n",
      "Train epoch[10/15], MiniBatch[95/99], Loss: 0.04445, Acc: 99.67105, LR: 0.00001                \n",
      "Eval epoch[10/15], MiniBatch[65/66], Loss: 0.07159, Acc: 96.16587, Epochs without improvement: 2\n",
      "\n",
      "Validation accuracy has not improved for the last 3 epochs, stopping training early at epoch 10/15\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Putting it all together now, we can launch the final training run!\n",
    "epochs =   15            # increase epochs to show off early stopper\n",
    "batch_sz = 128           # increase batch size for faster processing\n",
    "checkpoint_frequency = 3 # save model to a file every 3 epochs  \n",
    "\n",
    "\n",
    "# data loading with new batch size\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}\n",
    "\n",
    "network = get_resnet()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)  # Stochastic gradient descent optimiser\n",
    "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
    "network = network.to(device)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "stopper = EarlyStopper(patience=3, tolerance=0) # stop training if model accuracy is not better than the max so far 3 times in a row\n",
    "train_model_final(device, network, epochs, dataloaders, optim, lr_sch, writer, stopper, checkpoint_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8769390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch[165/165]\n",
      "Final test accuracy for 21000 examples: 97.47143\n"
     ]
    }
   ],
   "source": [
    "# Once you're happy with your trained model, you can load it from the latest checkpoint\n",
    "# that has been saved. The exact number will depend on when your training was stopped\n",
    "# so please check this in your `saved_models` directory before loading\n",
    "\n",
    "last_epoch = 10\n",
    "loaded_net = torch.load(f\"./saved_models/checkpoint_{last_epoch}.pth\")\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}\n",
    "\n",
    "# We define a testing function that is identical to the validation routine in our \n",
    "# training function. We're just using this as a substitute for deployment\n",
    "def test_model(device, model, dataloaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    test_dl = dataloaders['test']\n",
    "    total_steps = len(test_dl)\n",
    "    msg = \"\"\n",
    "    for batch_num, (image_batch, label_batch) in enumerate(test_dl):\n",
    "        batch_sz = len(image_batch)\n",
    "        label_batch = label_batch.to(device)\n",
    "        image_batch = image_batch.to(device).reshape(batch_sz, 1, 28, 28)\n",
    "        out = model(image_batch)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        correct += int(torch.eq(preds, label_batch).sum())\n",
    "        total += label_batch.shape[0]\n",
    "        if (batch_num + 1) % 5 == 0:\n",
    "            print(\" \" * len(msg), end='\\r')\n",
    "            msg = f'Testing batch[{batch_num + 1}/{total_steps}]'\n",
    "            print (msg, end='\\r' if batch_num < total_steps else \"\\n\", flush=True)\n",
    "    print(f\"\\nFinal test accuracy for {total} examples: {100 * correct/total:.5f}\")\n",
    "    \n",
    "    \n",
    "test_model(device, loaded_net, dataloaders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
