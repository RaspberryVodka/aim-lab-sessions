{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2836d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the variables, classes and functions we defined in the previous lessons\n",
    "from vars.week_3 import *\n",
    "from vars.week_4 import *\n",
    "from vars.week_5 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190c130",
   "metadata": {},
   "source": [
    "# 6. More complex modules\n",
    "\n",
    "## 6.1 Subclassing nn.Module\n",
    "So far, we've looked at how to put together simple models using Pytorch's `nn.Sequential` function. Remember, all it does is sequentially line up the layers and directs the output of one into the input of the next. However, there are cases where we don't want out data to flow in such a linear fashion. A good example is a Residual Network or ResNet, which applies shortcuts between its layers. This cannot be done using a simple `nn.Sequential` construction. In such cases, it becomes necessary to sub-class the `nn.Module` class and create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841b1eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 14, 14]           3,200\n",
      "         MaxPool2d-2             [-1, 64, 7, 7]               0\n",
      "       BatchNorm2d-3             [-1, 64, 7, 7]             128\n",
      "              ReLU-4             [-1, 64, 7, 7]               0\n",
      "            Conv2d-5             [-1, 64, 7, 7]          36,928\n",
      "       BatchNorm2d-6             [-1, 64, 7, 7]             128\n",
      "            Conv2d-7             [-1, 64, 7, 7]          36,928\n",
      "       BatchNorm2d-8             [-1, 64, 7, 7]             128\n",
      "          ResBlock-9             [-1, 64, 7, 7]               0\n",
      "           Conv2d-10             [-1, 64, 7, 7]          36,928\n",
      "      BatchNorm2d-11             [-1, 64, 7, 7]             128\n",
      "           Conv2d-12             [-1, 64, 7, 7]          36,928\n",
      "      BatchNorm2d-13             [-1, 64, 7, 7]             128\n",
      "         ResBlock-14             [-1, 64, 7, 7]               0\n",
      "           Conv2d-15            [-1, 128, 4, 4]           8,320\n",
      "      BatchNorm2d-16            [-1, 128, 4, 4]             256\n",
      "           Conv2d-17            [-1, 128, 4, 4]          73,856\n",
      "      BatchNorm2d-18            [-1, 128, 4, 4]             256\n",
      "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "         ResBlock-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "         ResBlock-26            [-1, 128, 4, 4]               0\n",
      "           Conv2d-27            [-1, 256, 2, 2]          33,024\n",
      "      BatchNorm2d-28            [-1, 256, 2, 2]             512\n",
      "           Conv2d-29            [-1, 256, 2, 2]         295,168\n",
      "      BatchNorm2d-30            [-1, 256, 2, 2]             512\n",
      "           Conv2d-31            [-1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-32            [-1, 256, 2, 2]             512\n",
      "         ResBlock-33            [-1, 256, 2, 2]               0\n",
      "           Conv2d-34            [-1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-35            [-1, 256, 2, 2]             512\n",
      "           Conv2d-36            [-1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-37            [-1, 256, 2, 2]             512\n",
      "         ResBlock-38            [-1, 256, 2, 2]               0\n",
      "           Conv2d-39            [-1, 512, 1, 1]         131,584\n",
      "      BatchNorm2d-40            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-41            [-1, 512, 1, 1]       1,180,160\n",
      "      BatchNorm2d-42            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-43            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-44            [-1, 512, 1, 1]           1,024\n",
      "         ResBlock-45            [-1, 512, 1, 1]               0\n",
      "           Conv2d-46            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-47            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-48            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-49            [-1, 512, 1, 1]           1,024\n",
      "         ResBlock-50            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-51            [-1, 512, 1, 1]               0\n",
      "          Flatten-52                  [-1, 512]               0\n",
      "           Linear-53                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,180,170\n",
      "Trainable params: 11,180,170\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.74\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 43.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample):\n",
    "        super().__init__()\n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        shortcut = self.shortcut(inp)\n",
    "        inp = nn.ReLU()(self.bn1(self.conv1(inp)))\n",
    "        inp = nn.ReLU()(self.bn2(self.conv2(inp)))\n",
    "        inp = inp + shortcut  # The magic bit that cannot be done with nn.Sequential!\n",
    "        return nn.ReLU()(inp)\n",
    "    \n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, resblock, outputs):\n",
    "        super().__init__()\n",
    "        self.layer0_conv = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.layer0_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0_bn   = nn.BatchNorm2d(64)\n",
    "        self.layer0_relu = nn.ReLU()\n",
    "\n",
    "        self.layer1_res1 = resblock(64, 64, downsample=False)\n",
    "        self.layer1_res2 = resblock(64, 64, downsample=False)\n",
    "\n",
    "        self.layer2_res1 = resblock(64, 128, downsample=True)\n",
    "        self.layer2_res2 = resblock(128, 128, downsample=False)\n",
    "\n",
    "        self.layer3_res1 = resblock(128, 256, downsample=True)\n",
    "        self.layer3_res2 = resblock(256, 256, downsample=False)\n",
    "\n",
    "        self.layer4_res1 = resblock(256, 512, downsample=True)\n",
    "        self.layer4_res2 = resblock(512, 512, downsample=False)\n",
    "\n",
    "        self.gap         = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flat        = nn.Flatten() \n",
    "        self.fc          = nn.Linear(512, outputs)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = self.layer0_conv(inp)\n",
    "        inp = self.layer0_pool(inp)\n",
    "        inp = self.layer0_bn(inp)\n",
    "        inp = self.layer0_relu(inp)\n",
    "        \n",
    "        inp = self.layer1_res1(inp)\n",
    "        inp = self.layer1_res2(inp)\n",
    "        \n",
    "        inp = self.layer2_res1(inp)\n",
    "        inp = self.layer2_res2(inp)\n",
    "        \n",
    "        inp = self.layer3_res1(inp)\n",
    "        inp = self.layer3_res2(inp)\n",
    "        \n",
    "        inp = self.layer4_res1(inp)\n",
    "        inp = self.layer4_res2(inp)\n",
    "            \n",
    "        inp = self.gap(inp)\n",
    "        inp = self.flat(inp)\n",
    "        inp = self.fc(inp)\n",
    "\n",
    "        return inp\n",
    "    \n",
    "\n",
    "# convenience function\n",
    "def get_resnet():\n",
    "    return ResNet(1, ResBlock, outputs=10)\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "summary(get_resnet(), input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not need to define a new train function as the only changes have been to the internal\n",
    "# structure of the model, not any of its inputs or outputs, so we can keep using\n",
    "# the old one - very handy!\n",
    "\n",
    "# loading data\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}\n",
    "\n",
    "# setting device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "network = get_resnet()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
    "network = network.to(device)\n",
    "train_model_gpu_lr_conv_valid(device, network, epochs, dataloaders, optim, lr_sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305a279",
   "metadata": {},
   "source": [
    "## 6.2 Early stopping\n",
    "Model training on a typical machine learning project is a time-consuming process. Sometimes, we may even want to run the same model multiple times with different parameters to see which ones work the best (a process known as hyper-parameter tuning). If a model is allowed to run for a long time despite not getting any better at learning, that's wasted time and computing power. We use early stopping to tackle this problem. This is a simple algorithm that just says whether or not the model should be stopped, based on its performance over time. \n",
    "\n",
    "To illustrate this concept, we define a class `EarlyStopper` that performs this check. Its should_stop method must be called every epoch with the current validation accuracy for that epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, tolerance=0):\n",
    "        self.patience = patience           # How many epochs in a row the model is allowed to underperform    \n",
    "        self.tolerance = tolerance         # How much leeway the model has (i.e. how close it can get to underperforming before it is counted as such)\n",
    "        self.epoch_counter = 0             # Keeping track of how many epochs in a row were failed \n",
    "        self.max_validation_acc = np.NINF  # Keeping track of best metric so far\n",
    "\n",
    "    def should_stop(self, validation_acc):\n",
    "        if validation_acc > self.max_validation_acc:\n",
    "            self.max_validation_acc = validation_acc\n",
    "            self.epoch_counter = 0\n",
    "        elif validation_acc < (self.max_validation_acc - self.tolerance):\n",
    "            self.epoch_counter += 1\n",
    "            if self.epoch_counter >= self.patience:\n",
    "                return True\n",
    "        return False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e758d07",
   "metadata": {},
   "source": [
    "## 6.3 Logging\n",
    "We will also be using a logging tool called `tensorboard` to keep track of our metrics and visualise the performance of our model as it goes through its training cyle. The output of a Pytorch training run is typically stored in a directory called `runs` somewhere in the project root. We need to pass this directory to `tensorboard` so that it can display the progress of our runs for us. Let's import this module now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b06783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428863c",
   "metadata": {},
   "source": [
    "## 6.4 Checkpointing\n",
    "Finally, it is typical in machine learning pipelines to save the progress of a long-running training session every so often. Since it is such a time consuming process, chances are, sooner or later, one of any number of things outside the control of the programmer might interrupt the training (power outage, alien attack etc, etc.). If such a thing were to happen in the middle of a training cyle, that could mean days or weeks of training lost in an instant. It's STRONGLY recommended that you implement some kind of checkpointing functionality in your ML training pipeline. We define a save function below for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b412ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Saves a model to file, and names it after the current epoch\n",
    "def save_checkpoint(model, epoch, save_dir):\n",
    "    filename = f\"checkpoint_{epoch}.pth\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af23dc",
   "metadata": {},
   "source": [
    "## 6.5 Final training loop \n",
    "Using all this information, we update our training function one last time.\n",
    "\n",
    "For those of you keeping score, this training function now supports:\n",
    "1. GPU training\n",
    "2. Adaptive learning rates\n",
    "3. Training and validation epochs\n",
    "4. Early stopping\n",
    "5. Logging metrics \n",
    "6. Model checkpointing\n",
    "\n",
    "To avoid ending up with a ridiculously long name for our training function (`train_model_gpu_lr_conv_valid_stopping_logging_checkpoint` or some such), we shall rename it one last time to something sensible. 3 additional arguments have been added: the summary writer, the early stopper that we just wrote, and how often we want to save checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_final(device, model, epochs, dataloaders, optimiser, lr_scheduler, writer, early_stopper, checkpoint_frequency):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):        \n",
    "        #######################TRAINING STEP###################################\n",
    "        model.train()  # set model to training mode \n",
    "        train_dl = dataloaders['train'] # select train dataloader\n",
    "        \n",
    "        total_steps_train = len(train_dl)\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        loss_train = 0\n",
    "        \n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(device)\n",
    "            image_batch = image_batch.to(device).reshape(batch_sz, 1, 28, 28) \n",
    "            output = model(image_batch)\n",
    "            loss_train = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            preds_train = torch.argmax(output, dim=1)\n",
    "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
    "            total_train += batch_sz\n",
    "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
    "            \n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_train}], Loss: {loss_train.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "        lr_scheduler.step()\n",
    "        ########################################################################\n",
    "        print(\"\") # Create newline between progress bars\n",
    "        #######################VALIDATION STEP##################################\n",
    "        model.eval()  # set model to evaluation mode. This is very important, we do not want to update model weights in eval mode\n",
    "        val_dl = dataloaders['val'] # select val dataloader\n",
    "        \n",
    "        total_steps_val = len(val_dl)\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        loss_val = 0\n",
    "        \n",
    "        for batch_num, (image_batch, label_batch) in enumerate(val_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(device)\n",
    "            image_batch = image_batch.to(device).reshape(batch_sz, 1, 28, 28) \n",
    "            \n",
    "            with torch.no_grad(): # no_grad disables gradient calculations, which are not needed when evaluating the model. This speeds up the calculations\n",
    "                output = model(image_batch)\n",
    "                loss_val = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "                preds_val = torch.argmax(output, dim=1)\n",
    "                correct_val += int(torch.eq(preds_val, label_batch).sum())\n",
    "                total_val += batch_sz\n",
    "                minibatch_accuracy_val = 100 * correct_val / total_val\n",
    "                \n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "                if (batch_num + 1) % 5 == 0:\n",
    "                    print(\" \" * len(msg), end='\\r')\n",
    "                    msg = f'Eval epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_val}], Loss: {loss_val.item():.5f}, Acc: {minibatch_accuracy_val:.5f}'\n",
    "                    if early_stopper.epoch_counter > 0:\n",
    "                        msg += f\", Epochs without improvement: {early_stopper.epoch_counter}\"\n",
    "                    print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "        ########################################################################\n",
    "        print(\"\")  # Create newline between progress bars\n",
    "        \n",
    "        # Log loss and accuracy metrics using the writer so we can see them in Tensorboard \n",
    "        epoch_train_acc = 100 * correct_train / total_train\n",
    "        epoch_val_acc = 100 * correct_val / total_val\n",
    "        \n",
    "        writer.add_scalar(f'Loss/train', loss_train, epoch)\n",
    "        writer.add_scalar(f'Acc/train', epoch_train_acc, epoch)\n",
    "        writer.add_scalar(f'Loss/val', loss_val, epoch)\n",
    "        writer.add_scalar(f'Acc/val', epoch_val_acc, epoch)\n",
    "        \n",
    "        # Check whether we need to save the model to a checkpoint file\n",
    "        if (epoch + 1) % checkpoint_frequency == 0:\n",
    "            save_checkpoint(model, epoch + 1, \"./saved_models\")\n",
    "\n",
    "        # Check whether we should stop the training based on the validation accuracy\n",
    "        if early_stopper.should_stop(epoch_val_acc):\n",
    "            print(f\"\\nValidation accuracy has not improved for the last {early_stopper.epoch_counter} epochs, stopping training early at epoch {epoch + 1}/{epochs}\")\n",
    "            # if stopping, we also want to save the checkpoint so we don't lose anything between the last save\n",
    "            save_checkpoint(model, epoch + 1, \"./saved_models\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66091303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together now, we can launch the final training run!\n",
    "epochs =   15            # increase epochs to show off early stopper\n",
    "batch_sz = 128           # increase batch size for faster processing\n",
    "checkpoint_frequency = 3 # save model to a file every 3 epochs  \n",
    "\n",
    "\n",
    "# data loading with new batch size\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}\n",
    "\n",
    "network = get_resnet()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)  # Stochastic gradient descent optimiser\n",
    "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
    "network = network.to(device)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "stopper = EarlyStopper(patience=3, tolerance=0) # stop training if model accuracy is not better than the max so far 3 times in a row\n",
    "train_model_final(device, network, epochs, dataloaders, optim, lr_sch, writer, stopper, checkpoint_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f345b0",
   "metadata": {},
   "source": [
    "## 6.6 Deployment\n",
    "Once you're happy with your trained model, you can load it from the latest checkpoint that has been saved. The exact number will depend on when your training was stopped so please check this in your `saved_models` directory before loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8769390",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = 10\n",
    "loaded_net = torch.load(f\"./saved_models/checkpoint_{last_epoch}.pth\")\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd99ab",
   "metadata": {},
   "source": [
    "## 6.7 Testing/Inference\n",
    "Congratulations! This is the point at which you've trained your model to your satisfaction and are ready to throw it into some real world applications. We can now use the test data split that we've been hanging onto for ages to see how the model does against it. First, we define a testing function (this is identical to the validation routine in our training function. We're just using this as a substitute for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba003979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(device, model, dataloaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    test_dl = dataloaders['test']\n",
    "    total_steps = len(test_dl)\n",
    "    msg = \"\"\n",
    "    for batch_num, (image_batch, label_batch) in enumerate(test_dl):\n",
    "        batch_sz = len(image_batch)\n",
    "        label_batch = label_batch.to(device)\n",
    "        image_batch = image_batch.to(device).reshape(batch_sz, 1, 28, 28)\n",
    "        out = model(image_batch)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        correct += int(torch.eq(preds, label_batch).sum())\n",
    "        total += label_batch.shape[0]\n",
    "        if (batch_num + 1) % 5 == 0:\n",
    "            print(\" \" * len(msg), end='\\r')\n",
    "            msg = f'Testing batch[{batch_num + 1}/{total_steps}]'\n",
    "            print (msg, end='\\r' if batch_num < total_steps else \"\\n\", flush=True)\n",
    "    print(f\"\\nFinal test accuracy for {total} examples: {100 * correct/total:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0deab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch[655/657]\n",
      "Final test accuracy for 21000 examples: 97.54762\n"
     ]
    }
   ],
   "source": [
    "test_model(device, loaded_net, dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79d2b1",
   "metadata": {},
   "source": [
    "## 6.8 Debrief: Image classification net\n",
    "\n",
    "Might be cool to live code a function that takes an image of a number from disk and uses our model to predict it - might do this if there is enough time left "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b1c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_tensor(filepath):\n",
    "    img = (\n",
    "            Image.open(filepath)   # load image\n",
    "                .convert('L')      # convert from RGBA to grayscale\n",
    "                .resize((28, 28), resample=PIL.Image.Resampling.BICUBIC)  # resize to what our network expects\n",
    "                \n",
    "          )\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor()\n",
    "    ])\n",
    "    img_tensor = transform(img)\n",
    "    img_tensor = torch.where((img_tensor <= 120), 0, 255)\n",
    "    return img_tensor.type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "def live_test_images(filepaths, model, device):\n",
    "    batch_sz = len(filepaths)\n",
    "    batch = torch.FloatTensor(batch_sz, 28, 28) \n",
    "    torch.cat([load_image_tensor(f) for f in filepaths], out=batch)\n",
    "    batch = batch.reshape(batch_sz, 1, 28, 28).to(device) # Only item in batch, 1 channel, 28 * 28 pixels\n",
    "    out = model(batch)\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    for i, p in enumerate(preds):\n",
    "        show_img(filepaths[i], p.item())\n",
    "\n",
    "def show_img(path, prediction):\n",
    "    img = load_image_tensor(path)\n",
    "    fig = plt.figure(figsize=(1., 1.))\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "    fig.axes[0].set_axis_off()\n",
    "    fig.suptitle(f\"Prediction for image: {prediction}\", y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d2acccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAB+CAYAAAC6cPTUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaWUlEQVR4nO2daVBTWdrH/zd7IAkgJCwCISLoKHFD2wU3UEFFp3RqZBStQqctmXbr7rHbmnGq3UbHmm5rSkutFp2p6SpHtIceHatcGtDWnkamHRe0ERVRwmZaE5TNsISQ837wza0EEsjGZp9fFR84Ofec557cf85yn/MchhBCQKFQ3IbT3wZQKIMdKiIKxUOoiCgUD6EiolA8hIqIQvEQKiIKxUOoiCgUD6EiolA8hIqIQvEQKiIKxUN4/W3ATw1XvKwYhulFSyjegoqoDykvL8fdu3dhMBh6zKtSqTBu3DhIJJI+sIziCVREfQQhBHfv3sVnn30GrVbbbV6GYbB06VIMGzaMimgQQEXUhxgMBmi1WlRVVXWbj2EYvHz5EiaTqY8so3gCXVigUDyEiohC8RA6nOtl2traUFdXh5aWFuj1+m6HaEKhEAEBARCJRAgKCgKPR7+ewQD9lnqZyspKZGdn4/Hjx9BoNKivr3eYV6lUIj09HTExMVCpVPD39+8zOynuQ0XUy+h0OuTm5uLGjRs9viOSy+VITk7GlClT6DuiQQQVUS/Q1taGyspK6HQ6FBcXo6mpyUZAHA4H4eHhCAsLA5fLZdPHjBkDqVRKBTTIoCLqBerq6pCdnY3c3Fw0NTWhsrLS5nOhUIjk5GSkp6dDLBaz6RKJBFFRUX1sLcVTqIi8DCEELS0tePz4scMhHJfLRUREBCZPngwfH59+sJLiTaiIvITJZEJJSQkePHiAH3/8ERqNxiU/OcrghYrISxiNRly5cgXHjh1DU1NTt6twlLcLKiIvYTab0dDQgOrqajQ3N9vNw+PxIBAIIJFIIBAI+thCSm9BRdSHjB49GklJSVAoFJg2bRr4fH5/m0TxAlREfQTDMBg1ahQyMzMREREBPp9PRfSWQEXkIa9fv0Z9fT0aGxtRX1/f7WICj8eDWCymK3JvGVREHmDZI5STk4Pnz5/jwYMHMBqN/W0WpY+hIvIQjUaDc+fOoaqqii5p/0ShInKDpqYmaDQaNDY24vHjx2htbXUoIKlUCpVKBZlMhtjYWAiFwj62ltLbUBG5gUajweHDh1FcXIza2lrU1dU5zKtSqbBx40ao1WoEBQUhICCgDy2l9AVURG7Q2NiI4uJifP/99z3mlUqlUKvVmDJlSh9YRukP6M5WCsVDqIgoFA+hw7legGEYCAQC8Pl8iMVicDj0t+pthoqoF/Dz88OcOXOgVqsRGRmJsLCw/jaJ0otQEfUCMpkMqampWLZsGet0Snl7oSJyEpPJhPr6ehgMBjx//hytra0O83I4HAiFQvj6+g6Ird6WiEOObObxePD396fRVt2EishJ6uvr8c9//hOFhYXQ6XSoqanpb5OcxjrikD0UCgXS0tIwbdq0Prbs7YCKyEkMBgMKCwtx6tQpmM3m/jbHJawjDtlDqVTinXfewdSpUwdEzznYoCJyEkIIzGbzgPaPM5vNqKmpgVartQkSaS/ikDWtra0oLS3F9evXbdKDg4OhVCrpnK4HqIjeItra2pCXl4fs7Gy0tLSw6fYiDlljiU6Ul5fHpnE4HKSkpCAzMxPBwcG9avdgh4poENO5ZzGZTKiqqsKNGzccblG3R1tbG548eYInT56waQzDIDo6mnWupcM8x1ARDVL0ej1u374NvV7PprW2tqK4uNhrR7KUl5fj7NmzCA4OxujRozFq1CgaH9wOtEUGKZWVlcjKysKdO3fYNLPZjMbGRrS3t3tcvmXDoUajgUwmw29+8xvExMRQEdmBtsggghACo9GI9vZ21NfXO3VgWHdYXJMs5XbuwQwGAwwGAxoaGqDX69nFCYFAQMVkBW2JQURDQwOuXLmC4uJiVFVV9XhsZXcIBAIkJCQgISEBBoMB+fn5uH//vt287e3tKCgogNlshlwux9y5c6FWq92u+22DimgQ0djYiAsXLiAnJwcmk8mjeA58Ph8JCQnYvHkzamtr8ezZM5SUlNhdBjcajSgsLMTNmzcRGRmJoUOHIi4uji42/D9URAMcQggaGhrQ2NiIqqoqvHr1CgaDocf3VRZXHkeuRz4+PpDL5ZBIJDAYDDanU9ijvb0d7e3taG1tpWfJdoKKaIBjCU984cIFvHr1Cvfu3XPqha+/vz+WLVuGadOm2d2KwePxMGrUKPoi1QtQEQ1w2tvbUVxcjJycHKd6IAu+vr6YNm0a0tPTHQ676HDMO1ARWVFQUABgYLi76PV6VFZWor6+HlVVVTCZTA4FJJfLoVQqbSIJhYaGIjg42CsbAq0PJQsPD4dcLqcCtIKKyIqPP/54QLi7EEJw+/ZtZGVlQavVQqvVOlxEYBgGEydOxLp16xASEsKmC4VCREREeMUe60PJ/P39vVbu2wIVkRU3btwAh8PB8OHD+y2SqcXRVafT4fbt26iurnaYl2EYcDgcyOVyxMfHu/VwE0K6HSIyDAMej4fIyEh6KJkDqIisWLVqFRiGQUJCQr88LBZXHp1Oh+vXr3fr/yaXyzFx4kTI5XK37HXmULJhw4Zh7NixCAoKglqtpi9YHUBbxYo9e/aAYRj4+PjA39+/z+u3uPLcvn0bzc3N3R4UplQqsW7dOsTHx7tlb0+HkjEMg7Fjx2Lr1q0IDw+HTCajp1g4gIrIisjISABvfqXb2tpgNpshEAhcXmAwm81obW3F69evwePxIBQKHU7w7bnyOBrCWUcR8vf3R1hYmFNDOLPZjLa2NnR0dLBpr1+/hl6v7/ZQMolEwi4mUBxDRWSHkpISXLlyBc3NzZgxY4bL26YbGhpw/vx5VFRUQKlUIiUlxeGD6Iorj7tRhGpqapCXl2cjTosXgjecVX/qUBF1ghCCBw8e4NixY6irqwOPx8OkSZNcKqOxsREXL15Ebm4upk6dCrVa7VBErrjyuBtFSKvVIjs7u8v2cIsXAsUzqIisqKioACEETU1N8PPzA8MwMBqNqKqqQm1trdMb3QghaGtrA/Bmj0/nmAyuuPIwDAM/Pz/IZDJERkZiyJAhDl15rMu1rrOmpgZ1dXVO2S8UChEQEACRSISgoCC6mOAEtIWs2LZtGzgcDoYOHYp169aBz+ejvLwc+/btQ0NDg9MuNz3hiiuPQCDAnDlzkJqaiiFDhmDs2LEOX3Ral2sdHqu2ttbp6ERKpRLp6emIiYmBSqXqlwWWwQYVkRWnT58GwzBYtWoVNmzYgICAAOzfvx9nzpxxyeWmJ1xx5eHz+VCr1Vi2bFmPcew6l2vBFbvlcjmSk5MxZcoU6pXgJFREVlgeNp1Oh1u3bkEmk6G6uhomkwkCgQBKpRJyuRwNDQ2orKxEU1OTR3V1Fz3I4srj7++PyMhI8Hg8m4fa+qAxC62trT26CNlDKBSy9zZmzBhIpVIqIBegIuoEIQS3bt2CTqcDj8djXW4UCgXS09ORnJyMH374AYcPH3a4ic1TrF15wsLCEBYW1mURwfqgMQtms7lbFyFHBAQEsPcmlUoRFRXljdv4yUBFZIfa2lrU1tbapIlEIsTExGDKlCno6OhwKeRu5x6nJ1cbAAgKCsKECRPYd1edr3HloLHuYBjG5t5oD+Q6VES9zIsXL5Cbm4unT5+yaa5E5bF2z7HO//jx4y5CdwUej8dG8AkJCYFKpaICchMqol6mqqoKWVlZNtsUXInKY+2eYx2Q0RKk3l0EAgGSkpKQmZkJqVRKV+E8gIrISaxdeYxGI4RCISQSCdrb22E0Gh0Oz4xGI168eOFyfSaTCc3NzWhsbOzRPccVLC9pZTIZFAoFIiIiqGe2hzBkIAeX7mO6G874+fkhKSkJcXFx4HA4bN579+7hm2++QUNDg1dtGTNmDBITEyEWi1FYWIj//ve/XvEuGDt2LJKSkqBQKDBt2jRMnTqVOpZ6CqGwAHD4xzAMEQqFxMfHhyQmJpJvv/2WNDU1kePHj5Pw8PBur3Xnj8fjEbFYTHx8fAifz/dKmQzDkBUrVpBHjx4Rg8FAjEZjfzf5WwEdzjkJsXLlaWpqwsuXL6HX62E2mxEWFgYOh8O63BAvdO4mk8lrUXWsXXlCQkIglUrpEM6L0OGcFc6uTsnlcowfPx5yuRxDhw5FbGwsGIbB+fPncfHiRVZsA4XY2FgbV56xY8dSEXkR2hO5gV6vR15eHjgcjo2LUEVFBXJzc/vbvC5QV57ehYrIAwghrIuQv78/OBwOJk+ezJ4HZH1iQ19gHZXHOhgjdeXpXehwzgp3HrKgoCBERkZCKpUiMTERiYmJePnyJY4ePWpzaFZfIBaLsXLlSqSnp0MsFrPpEokEUVFR9GDjXoL2RB5icRGSSCRITEzEhAkToNfrERgYCIZhuhyQ5c3frM6ip1F5+gcqIi/R3t6Oe/fu4fTp02hsbGSj51ii8gQFBaG8vBx379612abgLtblWhCJRDQqTz9AW9tLGI1GfPPNN7h58ybMZjMbPccSlWf8+PE4e/YsNBqNV0RkKXfChAlsGofDoVF5+gEqIi9B/n9rdmfPBaFQiJCQEERGRiIwMLDbXoLD4UAoFPZ4QgMANtqPxcub0n9QEQ0gwsPDkZyc7FQYrIiICKej/VB6FyqiAURYWBjS09MxefLkHvNyOBx6LMoAgYqol2lra8OPP/6IiooKtLS0sEHn7bkImUwm1NfXQ6/Xw9fXFwEBAU4N7Sj9C31PZEVvvIx0xUXIOu/06dOxbNkyBAYGet0minehPVEv44qLkHVeLpeL1NRUKqJBgOcnQFGcgljFSGAYpttez5KXDhIGB3Q4R6F4CO2JKBQPoSKiUDyEiohC8RAqIgrFQ6iIKBQPoSKiUDyEiohC8RAqIgrFQ6iIKBQPoSKiUDxk0IkoKioKq1evZv+/du0aGIbBtWvXvFYHwzDYuXOn18pzha+//hrjxo2DSCQCwzDsNvPe4IsvvgDDMKioqOi1On4KuCQiS6Nb/kQiEWJjY7Fx40a3Tj7oTy5evNhvQnHEy5cvkZaWBrFYjCNHjuDEiRPw9fXtb7PeSs6cOYNf/epXGDZsGHx8fDBixAhs2bLFvR8tVwJ3//3vfycAyO7du8mJEyfI8ePHSUZGBuFwOESlUhGDweCF8ODdo1QqSUZGBvt/R0cHaWlpIR0dHS6Vs2HDBuLo9ltaWkh7e7snZrrFpUuXCACSn5/fJ/WZTCbS0tJCzGZzn9Q3kAgMDCRqtZp88skn5Pjx42Tz5s1EIBCQkSNHkubmZpfKcms/0YIFCzBx4kQAwNq1axEYGIi//OUvOHfuHFasWGH3GoPB0Cu/qhwOByKRyKtlers8Z9HpdADg1QO3umt3Lpf7k905+9VXX2H27Nk2afHx8cjIyMDJkyexdu1ap8vyypwoKSkJwJvDeAFg9erVkEgkePr0KRYuXAipVIqVK1cCeHNY1oEDBzB69GiIRCIEBwcjMzOzy6lvhBDs2bMH4eHh8PHxQWJiIkpKSrrU7WhOdOPGDSxcuBABAQHw9fXFmDFjcPDgQda+I0eOAIDN8NSCvTlRUVERFixYAJlMBolEgjlz5nQ5L9Uy3L1+/Tp++9vfQi6Xw9fXF0uXLu0xpPDs2bORkZEBAJg0aRIYhrGZ++Xk5CA+Ph5isRhBQUFYtWoVnj17ZlNGd+1uD3tzoqioKCxatAjXrl3DxIkTIRaLoVar2fY9c+YM1Go1RCIR4uPjUVRUZFPmDz/8gNWrV2PYsGHsKRS//vWv8fLlyy71W+oQiUSIjo5GVlYWdu7caXev1T/+8Q/2/ocMGYLly5ejurraJk9zczMePXrk1DGcnQUEAEuXLgUAPHz4sMfrrfHKzlbLeaTWuzBNJhNSUlIwffp07N+/n43ImZmZiS+++AJr1qzB5s2b2VOwi4qKcP36dTZm2vbt27Fnzx4sXLgQCxcuxJ07d5CcnOzUydj5+flYtGgRQkND8f777yMkJAQPHz7E+fPn8f777yMzMxNarRb5+fk4ceJEj+WVlJRgxowZkMlk2Lp1K/h8PrKysjB79mx8++23XQKLbNq0CQEBAdixYwcqKipw4MABbNy4EV9++aXDOv7whz9gxIgROHbsGHbv3g2VSoXo6GgAYNtr0qRJ2LdvH168eIGDBw/i+vXrKCoqsum5HLW7Kzx58gTp6enIzMzEqlWrsH//fixevBhHjx7Ftm3bsH79egDAvn37kJaWhtLSUnA4b36P8/PzUV5ejjVr1iAkJAQlJSU4duwYSkpK8P3337MCKSoqwvz58xEaGopdu3aho6MDu3fvhlwu72LP3r178cknnyAtLQ1r166FXq/HoUOHMHPmTJv7/9///ofExETs2LHDrfnu8+fPAcAmIKZTuDL2s8yJLl++TPR6PamurianT58mgYGBRCwWk5qaGkIIIRkZGQQA+d3vfmdz/XfffUcAkJMnT9qkf/311zbpOp2OCAQCkpqaajNe37ZtGwFgMye6evUqAUCuXr1KCHkzzlepVESpVJK6ujqbeqzL6m5OBIDs2LGD/X/JkiVEIBCQp0+fsmlarZZIpVIyc+bMLu0zd+5cm7o+/PBDwuVySX19vd36Ol9/8+ZNNs1oNBKFQkHi4uJIS0sLm37+/HkCgGzfvp1Nc9TuPdWn0WjYNKVSSQCQwsJCNi03N5cAIGKxmFRWVrLpWVlZNm1PCLE7nzh16hQBQP7zn/+waYsXLyY+Pj7k2bNnbFpZWRnh8Xg230tFRQXhcrlk7969NmUWFxcTHo9nk255Fqy/O1d49913CZfLJY8fP3bpOreGc3PnzoVcLkdERASWL18OiUSCs2fPYujQoTb53nvvPZv/c3Jy4Ofnh3nz5rExrGtraxEfHw+JRIKrV68CAC5fvgyj0YhNmzbZdO0ffPBBj7YVFRVBo9Hggw8+6DK3cCcQSUdHB/Ly8rBkyRIMGzaMTQ8NDUV6ejoKCgrQ2Nhoc826dets6poxYwY6OjpQWVnpcv23bt2CTqfD+vXrbeZqqampGDlyJC5cuNDlms7t7iqjRo3C1KlT2f8tPW1SUpJNsEhLenl5OZtmHUi/tbUVtbW1mDJlCgDgzp07AN606eXLl7FkyRKb2HnDhw/HggULbGw5c+YMzGYz0tLSbJ6ZkJAQxMTEsM8M8GaIRghxqxfKzs7G3/72N2zZsgUxMTEuXevWcO7IkSOIjY0Fj8dDcHAwRowYwXbnbME8HsLDw23SysrK0NDQAIVCYbdcy8Ta8rB1vhm5XI6AgIBubbMMLePi4py/oW7Q6/Vobm7GiBEjunz2s5/9DGazGdXV1Rg9ejSb3jkqqcVmd077trSFvfpHjhyJgoICmzR77e4qne338/MDgC5BJS3p1vf16tUr7Nq1C6dPn2a/TwuW6LA6nQ4tLS0YPnx4l7o7p5WVlYEQ4vDB9kbI5O+++w7vvvsuUlJSsHfvXpevd0tE77zzDrs65wihUNhFWGazGQqFAidPnrR7jb3x8GDE0YoX6YNwFvba3VUc2e/MfaWlpaGwsBAff/wxxo0bB4lEArPZjPnz58NsNrtsi9lsBsMwuHTpkt36PT0u5t69e/j5z3+OuLg4fPXVV24dBtCnIbOio6Nx+fJlJCQk2HT7nVEqlQDe/ApZD6H0en2Pv+aWyfj9+/cxd+5ch/lcOVrSx8cHpaWlXT579OgROByOU2F/3cXSFqWlpewqqIXS0lL284FAXV0drly5gl27dmH79u1sellZmU0+hUIBkUiEJ0+edCmjc1p0dDQIIVCpVIiNjfWqvU+fPsX8+fOhUChw8eJFtwXZp24/aWlp6OjowB//+Mcun1mifwJv5lx8Ph+HDh2y+ZU7cOBAj3VMmDABKpUKBw4c6PL22bosy7uTnt5Qc7lcJCcn49y5czZLwS9evEB2djamT58OmUzWo13uMnHiRCgUChw9etQm0OOlS5fw8OFDpKam9lrdrmLpKTr3uJ2/Ny6Xi7lz5+Lf//43tFotm/7kyRNcunTJJu8vfvELcLlc7Nq1q0u5hBCbpXNXlrifP3+O5ORkcDgc5ObmejQK6tOeaNasWcjMzMS+fftw9+5dJCcng8/no6ysDDk5OTh48CB++ctfQi6X46OPPsK+ffuwaNEiLFy4EEVFRbh06VKPy48cDgeff/45Fi9ejHHjxmHNmjUIDQ3Fo0ePUFJSwgZMjI+PBwBs3rwZKSkp4HK5WL58ud0y9+zZg/z8fEyfPh3r168Hj8dDVlYW2tra8Omnn3q3kTrB5/Px5z//GWvWrMGsWbOwYsUKdok7KioKH374Ya/W7woymQwzZ87Ep59+ivb2dgwdOhR5eXns+0Nrdu7ciby8PCQkJOC9995DR0cHDh8+jLi4ONy9e5fNFx0djT179uD3v/89KioqsGTJEkilUmg0Gpw9exbr1q3DRx99BMC1Je758+ejvLwcW7duRUFBgc3cMjg4GPPmzXP+xl1ZyrO3BGuPjIwM4uvr6/DzY8eOkfj4eCIWi4lUKiVqtZps3bqVaLVaNk9HRwfZtWsXCQ0NJWKxmMyePZvcv3+/i9tP5yVuCwUFBWTevHlEKpUSX19fMmbMGHLo0CH2c5PJRDZt2kTkcjlhGMZmWRV2lknv3LlDUlJSiEQiIT4+PiQxMdFmGbi79nFkY2e6a98vv/ySjB8/ngiFQjJkyBCycuVK9pWChZ7a3VF9nZe4U1NTu+QFQDZs2GCTptFoCADy2WefsWk1NTVk6dKlxN/fn/j5+ZFly5YRrVZrt02vXLlCxo8fTwQCAYmOjiZ//etfyZYtW4hIJOpS/7/+9S8yffp04uvrS3x9fcnIkSPJhg0bSGlpKZvHlSVuAA7/Zs2a1eP11tDgjZQBxZIlS1BSUtJlHjWQGXRbIShvDy0tLTb/l5WV4eLFi3ZdcgYytCei9BuhoaGsn11lZSU+//xztLW1oaioyOUXnv0JPRWC0m/Mnz8fp06dwvPnzyEUCjF16lT86U9/GlQCAmhPRKF4DJ0TUSgeQkVEoXgIFRGF4iFURBSKh1ARUSgeQkVEoXgIFRGF4iFURBSKh1ARUSge8n+c07RxLpD+UAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "live_test_images([\"../data/6.png\", \"../data/4.png\"], loaded_net, device)\n",
    "# show_img(\"../data/6.png\", 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
