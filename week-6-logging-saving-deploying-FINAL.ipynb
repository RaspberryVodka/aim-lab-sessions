{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2836d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kavi/.virtualenvs/aim-lab-sessions/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Get all the variables, classes and functions we defined in the previous lessons\n",
    "from vars.week_3 import *\n",
    "from vars.week_4 import *\n",
    "from vars.week_5 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190c130",
   "metadata": {},
   "source": [
    "# 6. More complex modules\n",
    "\n",
    "## 6.1 Subclassing nn.Module\n",
    "So far, we've looked at how to put together simple models using Pytorch's `nn.Sequential` function. Remember, all it does is sequentially line up the layers and directs the output of one into the input of the next. However, there are cases where we don't want out data to flow in such a linear fashion. A good example is a Residual Network or ResNet, which applies shortcuts between its layers. This cannot be done using a simple `nn.Sequential` construction. In such cases, it becomes necessary to sub-class the `nn.Module` class and create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841b1eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 14, 14]           3,200\n",
      "         MaxPool2d-2             [-1, 64, 7, 7]               0\n",
      "       BatchNorm2d-3             [-1, 64, 7, 7]             128\n",
      "              ReLU-4             [-1, 64, 7, 7]               0\n",
      "            Conv2d-5             [-1, 64, 7, 7]          36,928\n",
      "       BatchNorm2d-6             [-1, 64, 7, 7]             128\n",
      "            Conv2d-7             [-1, 64, 7, 7]          36,928\n",
      "       BatchNorm2d-8             [-1, 64, 7, 7]             128\n",
      "          ResBlock-9             [-1, 64, 7, 7]               0\n",
      "           Conv2d-10             [-1, 64, 7, 7]          36,928\n",
      "      BatchNorm2d-11             [-1, 64, 7, 7]             128\n",
      "           Conv2d-12             [-1, 64, 7, 7]          36,928\n",
      "      BatchNorm2d-13             [-1, 64, 7, 7]             128\n",
      "         ResBlock-14             [-1, 64, 7, 7]               0\n",
      "           Conv2d-15            [-1, 128, 4, 4]           8,320\n",
      "      BatchNorm2d-16            [-1, 128, 4, 4]             256\n",
      "           Conv2d-17            [-1, 128, 4, 4]          73,856\n",
      "      BatchNorm2d-18            [-1, 128, 4, 4]             256\n",
      "           Conv2d-19            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "         ResBlock-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "         ResBlock-26            [-1, 128, 4, 4]               0\n",
      "           Conv2d-27            [-1, 256, 2, 2]          33,024\n",
      "      BatchNorm2d-28            [-1, 256, 2, 2]             512\n",
      "           Conv2d-29            [-1, 256, 2, 2]         295,168\n",
      "      BatchNorm2d-30            [-1, 256, 2, 2]             512\n",
      "           Conv2d-31            [-1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-32            [-1, 256, 2, 2]             512\n",
      "         ResBlock-33            [-1, 256, 2, 2]               0\n",
      "           Conv2d-34            [-1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-35            [-1, 256, 2, 2]             512\n",
      "           Conv2d-36            [-1, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-37            [-1, 256, 2, 2]             512\n",
      "         ResBlock-38            [-1, 256, 2, 2]               0\n",
      "           Conv2d-39            [-1, 512, 1, 1]         131,584\n",
      "      BatchNorm2d-40            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-41            [-1, 512, 1, 1]       1,180,160\n",
      "      BatchNorm2d-42            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-43            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-44            [-1, 512, 1, 1]           1,024\n",
      "         ResBlock-45            [-1, 512, 1, 1]               0\n",
      "           Conv2d-46            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-47            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-48            [-1, 512, 1, 1]       2,359,808\n",
      "      BatchNorm2d-49            [-1, 512, 1, 1]           1,024\n",
      "         ResBlock-50            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-51            [-1, 512, 1, 1]               0\n",
      "          Flatten-52                  [-1, 512]               0\n",
      "           Linear-53                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,180,170\n",
      "Trainable params: 11,180,170\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.74\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 43.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample):\n",
    "        super().__init__()\n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        shortcut = self.shortcut(inp)\n",
    "        inp = nn.ReLU()(self.bn1(self.conv1(inp)))\n",
    "        inp = nn.ReLU()(self.bn2(self.conv2(inp)))\n",
    "        inp = inp + shortcut  # The magic bit that cannot be done with nn.Sequential!\n",
    "        return nn.ReLU()(inp)\n",
    "    \n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, resblock, outputs):\n",
    "        super().__init__()\n",
    "        self.layer0_conv = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.layer0_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0_bn   = nn.BatchNorm2d(64)\n",
    "        self.layer0_relu = nn.ReLU()\n",
    "\n",
    "        self.layer1_res1 = resblock(64, 64, downsample=False)\n",
    "        self.layer1_res2 = resblock(64, 64, downsample=False)\n",
    "\n",
    "        self.layer2_res1 = resblock(64, 128, downsample=True)\n",
    "        self.layer2_res2 = resblock(128, 128, downsample=False)\n",
    "\n",
    "        self.layer3_res1 = resblock(128, 256, downsample=True)\n",
    "        self.layer3_res2 = resblock(256, 256, downsample=False)\n",
    "\n",
    "        self.layer4_res1 = resblock(256, 512, downsample=True)\n",
    "        self.layer4_res2 = resblock(512, 512, downsample=False)\n",
    "\n",
    "        self.gap         = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flat        = nn.Flatten() \n",
    "        self.fc          = nn.Linear(512, outputs)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = self.layer0_conv(inp)\n",
    "        inp = self.layer0_pool(inp)\n",
    "        inp = self.layer0_bn(inp)\n",
    "        inp = self.layer0_relu(inp)\n",
    "        \n",
    "        inp = self.layer1_res1(inp)\n",
    "        inp = self.layer1_res2(inp)\n",
    "        \n",
    "        inp = self.layer2_res1(inp)\n",
    "        inp = self.layer2_res2(inp)\n",
    "        \n",
    "        inp = self.layer3_res1(inp)\n",
    "        inp = self.layer3_res2(inp)\n",
    "        \n",
    "        inp = self.layer4_res1(inp)\n",
    "        inp = self.layer4_res2(inp)\n",
    "            \n",
    "        inp = self.gap(inp)\n",
    "        inp = self.flat(inp)\n",
    "        inp = self.fc(inp)\n",
    "\n",
    "        return inp\n",
    "    \n",
    "\n",
    "# convenience function\n",
    "def get_resnet():\n",
    "    return ResNet(1, ResBlock, outputs=10)\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "summary(get_resnet(), input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dffdbdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[1/5], MiniBatch[390/394], Loss: 0.08642, Acc: 92.29167, LR: 0.00500\n",
      "Eval epoch[1/5], MiniBatch[260/263], Loss: 0.02110, Acc: 96.52644              \n",
      "Train epoch[2/5], MiniBatch[390/394], Loss: 0.02194, Acc: 98.46955, LR: 0.00250\n",
      "Eval epoch[2/5], MiniBatch[260/263], Loss: 0.00352, Acc: 97.83654              \n",
      "Train epoch[3/5], MiniBatch[390/394], Loss: 0.00522, Acc: 99.35096, LR: 0.00125\n",
      "Eval epoch[3/5], MiniBatch[260/263], Loss: 0.11436, Acc: 98.17308              \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m lr_sch \u001b[38;5;241m=\u001b[39m ExponentialLR(optim, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[1;32m     17\u001b[0m network \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain_model_gpu_lr_conv_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_sch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/phd/teaching/aim-lab-sessions/vars/week_5.py:83\u001b[0m, in \u001b[0;36mtrain_model_gpu_lr_conv_valid\u001b[0;34m(model, epochs, dataloaders, optimiser, lr_scheduler)\u001b[0m\n\u001b[1;32m     80\u001b[0m correct_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     81\u001b[0m total_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_num, (image_batch, label_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     84\u001b[0m     batch_sz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(image_batch)\n\u001b[1;32m     85\u001b[0m     label_batch \u001b[38;5;241m=\u001b[39m label_batch\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m~/.virtualenvs/aim-lab-sessions/lib/python3.8/site-packages/torch/utils/data/dataloader.py:444\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/aim-lab-sessions/lib/python3.8/site-packages/torch/utils/data/dataloader.py:390\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/aim-lab-sessions/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1077\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1070\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1077\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/context.py:277\u001b[0m, in \u001b[0;36mForkProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_fork\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/popen_fork.py:16\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush_std_streams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/util.py:435\u001b[0m, in \u001b[0;36m_flush_std_streams\u001b[0;34m()\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flush_std_streams\u001b[39m():\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m         \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/aim-lab-sessions/lib/python3.8/site-packages/ipykernel/iostream.py:480\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:270\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;124;03m\"\"\"Wait until notified or until a timeout occurs.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    If the calling thread has not acquired the lock when this method is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_owned():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We do not need to define a new train function as the only changes have been to the internal\n",
    "# structure of the model, not any of its inputs or outputs, so we can keep using\n",
    "# the old one - very handy!\n",
    "\n",
    "# loading data\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}\n",
    "\n",
    "\n",
    "network = get_resnet()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
    "network = network.to(DEVICE)\n",
    "train_model_gpu_lr_conv_valid(network, epochs, dataloaders, optim, lr_sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305a279",
   "metadata": {},
   "source": [
    "## 6.2 Early stopping\n",
    "Model training on a typical machine learning project is a time-consuming process. Sometimes, we may even want to run the same model multiple times with different parameters to see which ones work the best (a process known as hyper-parameter tuning). If a model is allowed to run for a long time despite not getting any better at learning, that's wasted time and computing power. We use early stopping to tackle this problem. This is a simple algorithm that just says whether or not the model should be stopped, based on its performance over time. \n",
    "\n",
    "To illustrate this concept, we define a class `EarlyStopper` that performs this check. Its should_stop method must be called every epoch with the current validation accuracy for that epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2847fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, tolerance=0):\n",
    "        self.patience = patience           # How many epochs in a row the model is allowed to underperform    \n",
    "        self.tolerance = tolerance         # How much leeway the model has (i.e. how close it can get to underperforming before it is counted as such)\n",
    "        self.epoch_counter = 0             # Keeping track of how many epochs in a row were failed \n",
    "        self.max_validation_acc = np.NINF  # Keeping track of best metric so far\n",
    "\n",
    "    def should_stop(self, validation_acc):\n",
    "        if validation_acc > self.max_validation_acc:\n",
    "            self.max_validation_acc = validation_acc\n",
    "            self.epoch_counter = 0\n",
    "        elif validation_acc < (self.max_validation_acc - self.tolerance):\n",
    "            self.epoch_counter += 1\n",
    "            if self.epoch_counter >= self.patience:\n",
    "                return True\n",
    "        return False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e758d07",
   "metadata": {},
   "source": [
    "## 6.3 Logging\n",
    "We will also be using a logging tool called `tensorboard` to keep track of our metrics and visualise the performance of our model as it goes through its training cyle. The output of a Pytorch training run is typically stored in a directory called `runs` somewhere in the project root. We need to pass this directory to `tensorboard` so that it can display the progress of our runs for us. Let's import this module now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b06783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428863c",
   "metadata": {},
   "source": [
    "## 6.4 Checkpointing\n",
    "Finally, it is typical in machine learning pipelines to save the progress of a long-running training session every so often. Since it is such a time consuming process, chances are, sooner or later, one of any number of things outside the control of the programmer might interrupt the training (power outage, alien attack etc, etc.). If such a thing were to happen in the middle of a training cyle, that could mean days or weeks of training lost in an instant. It's STRONGLY recommended that you implement some kind of checkpointing functionality in your ML training pipeline. We define a save function below for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b412ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Saves a model to file, and names it after the current epoch\n",
    "def save_checkpoint(model, epoch, save_dir):\n",
    "    filename = f\"checkpoint_{epoch}.pth\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af23dc",
   "metadata": {},
   "source": [
    "## 6.5 Final training loop \n",
    "Using all this information, we update our training function one last time.\n",
    "\n",
    "For those of you keeping score, this training function now supports:\n",
    "1. GPU training\n",
    "2. Adaptive learning rates\n",
    "3. Training and validation epochs\n",
    "4. Early stopping\n",
    "5. Logging metrics \n",
    "6. Model checkpointing\n",
    "\n",
    "To avoid ending up with a ridiculously long name for our training function (`train_model_gpu_lr_conv_valid_stopping_logging_checkpoint` or some such), we shall rename it one last time to something sensible. 3 additional arguments have been added: the summary writer, the early stopper that we just wrote, and how often we want to save checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19f3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_final(model, epochs, dataloaders, optimiser, lr_scheduler, writer, early_stopper, checkpoint_frequency):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):        \n",
    "        #######################TRAINING STEP###################################\n",
    "        model.train()  # set model to training mode \n",
    "        train_dl = dataloaders['train'] # select train dataloader\n",
    "        \n",
    "        total_steps_train = len(train_dl)\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        loss_train = 0\n",
    "        \n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            image_batch = image_batch.to(DEVICE).reshape(batch_sz, 1, 28, 28) \n",
    "            output = model(image_batch)\n",
    "            loss_train = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            preds_train = torch.argmax(output, dim=1)\n",
    "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
    "            total_train += batch_sz\n",
    "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
    "            \n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_train}], Loss: {loss_train.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "        lr_scheduler.step()\n",
    "        ########################################################################\n",
    "        print(\"\") # Create newline between progress bars\n",
    "        #######################VALIDATION STEP##################################\n",
    "        model.eval()  # set model to evaluation mode. This is very important, we do not want to update model weights in eval mode\n",
    "        val_dl = dataloaders['val'] # select val dataloader\n",
    "        \n",
    "        total_steps_val = len(val_dl)\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        loss_val = 0\n",
    "        \n",
    "        for batch_num, (image_batch, label_batch) in enumerate(val_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            image_batch = image_batch.to(DEVICE).reshape(batch_sz, 1, 28, 28) \n",
    "            \n",
    "            with torch.no_grad(): # no_grad disables gradient calculations, which are not needed when evaluating the model. This speeds up the calculations\n",
    "                output = model(image_batch)\n",
    "                loss_val = nn.CrossEntropyLoss()(output, label_batch)\n",
    "\n",
    "                preds_val = torch.argmax(output, dim=1)\n",
    "                correct_val += int(torch.eq(preds_val, label_batch).sum())\n",
    "                total_val += batch_sz\n",
    "                minibatch_accuracy_val = 100 * correct_val / total_val\n",
    "                \n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "                if (batch_num + 1) % 5 == 0:\n",
    "                    print(\" \" * len(msg), end='\\r')\n",
    "                    msg = f'Eval epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_val}], Loss: {loss_val.item():.5f}, Acc: {minibatch_accuracy_val:.5f}'\n",
    "                    if early_stopper.epoch_counter > 0:\n",
    "                        msg += f\", Epochs without improvement: {early_stopper.epoch_counter}\"\n",
    "                    print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "                #### Fancy printing stuff, you can ignore this! ######\n",
    "        ########################################################################\n",
    "        print(\"\")  # Create newline between progress bars\n",
    "        \n",
    "        # Log loss and accuracy metrics using the writer so we can see them in Tensorboard \n",
    "        epoch_train_acc = 100 * correct_train / total_train\n",
    "        epoch_val_acc = 100 * correct_val / total_val\n",
    "        \n",
    "        writer.add_scalar(f'Loss/train', loss_train, epoch)\n",
    "        writer.add_scalar(f'Acc/train', epoch_train_acc, epoch)\n",
    "        writer.add_scalar(f'Loss/val', loss_val, epoch)\n",
    "        writer.add_scalar(f'Acc/val', epoch_val_acc, epoch)\n",
    "        \n",
    "        # Check whether we need to save the model to a checkpoint file\n",
    "        if (epoch + 1) % checkpoint_frequency == 0:\n",
    "            save_checkpoint(model, epoch + 1, \"./saved_models\")\n",
    "\n",
    "        # Check whether we should stop the training based on the validation accuracy\n",
    "        if early_stopper.should_stop(epoch_val_acc):\n",
    "            print(f\"\\nValidation accuracy has not improved for the last {early_stopper.epoch_counter} epochs, stopping training early at epoch {epoch + 1}/{epochs}\")\n",
    "            # if stopping, we also want to save the checkpoint so we don't lose anything between the last save\n",
    "            save_checkpoint(model, epoch + 1, \"./saved_models\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66091303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together now, we can launch the final training run!\n",
    "epochs =   15            # increase epochs to show off early stopper\n",
    "batch_sz = 128           # increase batch size for faster processing\n",
    "checkpoint_frequency = 3 # save model to a file every 3 epochs  \n",
    "\n",
    "\n",
    "# data loading with new batch size\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}\n",
    "\n",
    "network = get_resnet()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)  # Stochastic gradient descent optimiser\n",
    "lr_sch = ExponentialLR(optim, gamma=gamma)\n",
    "network = network.to(DEVICE)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "stopper = EarlyStopper(patience=3, tolerance=0) # stop training if model accuracy is not better than the max so far 3 times in a row\n",
    "train_model_final(network, epochs, dataloaders, optim, lr_sch, writer, stopper, checkpoint_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f345b0",
   "metadata": {},
   "source": [
    "## 6.6 Deployment\n",
    "Once you're happy with your trained model, you can load it from the latest checkpoint that has been saved. The exact number will depend on when your training was stopped so please check this in your `saved_models` directory before loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8769390",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = 10\n",
    "loaded_net = torch.load(f\"./saved_models/checkpoint_{last_epoch}.pth\")\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "dataloaders = {\n",
    "    'train': train_dl,\n",
    "    'val': val_dl,\n",
    "    'test': test_dl\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd99ab",
   "metadata": {},
   "source": [
    "## 6.7 Testing/Inference\n",
    "Congratulations! This is the point at which you've trained your model to your satisfaction and are ready to throw it into some real world applications. We can now use the test data split that we've been hanging onto for ages to see how the model does against it. First, we define a testing function (this is identical to the validation routine in our training function. We're just using this as a substitute for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba003979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloaders):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    test_dl = dataloaders['test']\n",
    "    total_steps = len(test_dl)\n",
    "    msg = \"\"\n",
    "    for batch_num, (image_batch, label_batch) in enumerate(test_dl):\n",
    "        batch_sz = len(image_batch)\n",
    "        label_batch = label_batch.to(DEVICE)\n",
    "        image_batch = image_batch.to(DEVICE).reshape(batch_sz, 1, 28, 28)\n",
    "        out = model(image_batch)\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        correct += int(torch.eq(preds, label_batch).sum())\n",
    "        total += label_batch.shape[0]\n",
    "        if (batch_num + 1) % 5 == 0:\n",
    "            print(\" \" * len(msg), end='\\r')\n",
    "            msg = f'Testing batch[{batch_num + 1}/{total_steps}]'\n",
    "            print (msg, end='\\r' if batch_num < total_steps else \"\\n\", flush=True)\n",
    "    print(f\"\\nFinal test accuracy for {total} examples: {100 * correct/total:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0deab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(loaded_net, dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac58f42",
   "metadata": {},
   "source": [
    "# 6.8 Pytorch Lightning\n",
    "Pytorch lightning is a framework that further simplifies the process of building a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c5e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kavi/.virtualenvs/aim-lab-sessions/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5307ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlResNet(pl.LightningModule):\n",
    "    def __init__(self, in_channels, outputs):\n",
    "        super().__init__()\n",
    "        # model arch goes here\n",
    "        self.model = ResNet(in_channels, ResBlock, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return SGD(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image_batch, label_batch = batch\n",
    "        batch_sz = len(image_batch)\n",
    "        image_batch = image_batch.reshape(batch_sz, 1, 28, 28) \n",
    "        output = model(image_batch)\n",
    "        loss = nn.CrossEntropyLoss()(output, label_batch)\n",
    "        \n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        correct = int(torch.eq(preds, label_batch).sum())\n",
    "        minibatch_acc = 100 * correct / batch_sz\n",
    "        \n",
    "        self.log(\"train_acc\", minibatch_acc)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image_batch, label_batch = batch\n",
    "        batch_sz = len(image_batch)\n",
    "        image_batch = image_batch.reshape(batch_sz, 1, 28, 28) \n",
    "        output = model(image_batch)\n",
    "        loss = nn.CrossEntropyLoss()(output, label_batch)\n",
    "        \n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        correct = int(torch.eq(preds, label_batch).sum())\n",
    "        minibatch_acc = 100 * correct / batch_sz\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', minibatch_acc)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cccfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)\n",
    "\n",
    "model = PlResNet(in_channels=1, outputs=10)\n",
    "\n",
    "# define callbacks\n",
    "# saves last-K checkpoints based on \"global_step\" metric\n",
    "# make sure you log it inside your LightningModule\n",
    "# NOTE: Talk a little bit about what callback functions are\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=5,\n",
    "                                      monitor=\"val_loss\",\n",
    "                                      mode=\"min\",\n",
    "                                      dirpath=\"./saved_models\",\n",
    "                                      filename=\"mnist-{epoch:02d}-{val_loss:.2f}\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_acc\", \n",
    "                                    min_delta=0.00, \n",
    "                                    patience=3, \n",
    "                                    verbose=False, \n",
    "                                    mode=\"max\")\n",
    "\n",
    "# train\n",
    "trainer = pl.Trainer(accelerator='gpu', \n",
    "                     devices=1, \n",
    "                     precision=16, \n",
    "                     limit_train_batches=0.5, \n",
    "                     callbacks=[early_stop_callback,\n",
    "                                checkpoint_callback]) \n",
    "trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79d2b1",
   "metadata": {},
   "source": [
    "## 6.8 Debrief: Image classification net\n",
    "\n",
    "Might be cool to live code a function that takes an image of a number from disk and uses our model to predict it - might do this if there is enough time left "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b1c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_file(filepath):\n",
    "    img = (\n",
    "            Image.open(filepath)   # load image\n",
    "                .convert('L')      # convert from RGBA to grayscale\n",
    "                .resize((28, 28), resample=PIL.Image.Resampling.BICUBIC)  # resize to what our network expects\n",
    "                \n",
    "          )\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor()\n",
    "    ])\n",
    "    img_tensor = transform(img)\n",
    "    img_tensor = torch.where((img_tensor <= 120), 0, 255)\n",
    "    return img_tensor.type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "def live_test_images(filepaths, model):\n",
    "    batch_sz = len(filepaths)\n",
    "    batch = torch.FloatTensor(batch_sz, 28, 28) \n",
    "    torch.cat([load_from_file(f) for f in filepaths], out=batch)\n",
    "    batch = batch.reshape(batch_sz, 1, 28, 28).to(DEVICE) # Only item in batch, 1 channel, 28 * 28 pixels\n",
    "    out = model(batch)\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    for i, p in enumerate(preds):\n",
    "        show_img(filepaths[i], p.item())\n",
    "\n",
    "def show_img(path, prediction):\n",
    "    img = load_from_file(path)\n",
    "    fig = plt.figure(figsize=(1., 1.))\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "    plt.tick_params(\n",
    "        axis='both',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelbottom=False, # labels along the bottom edge are off\n",
    "        labelleft=False\n",
    "    )\n",
    "    fig.suptitle(f\"Prediction for image: {prediction}\", y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2acccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_test_images([\"../data/6.png\", \"../data/4.png\"], loaded_net)\n",
    "# show_img(\"../data/6.png\", 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
