{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the variables, classes and functions we defined in the previous lessons\n",
    "from vars.week_3 import *\n",
    "from vars.week_4 import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bbfc9c",
   "metadata": {},
   "source": [
    "# 5. Improving the training loop\n",
    "## 5.1 Adaptive learning rate\n",
    "With the device selection out of the way, let's have a look at how we can improve the model's performance by modifiying one of its hyperparameters. You can think of hyperparameters as arguments to the model's training function. The reason they're not simply called parameters is because that word is reserved for the weights and biases within the network itself. In particular, we'll look at the model's learning rate.  \n",
    "\n",
    "From the lectures, you may remember that the learning rate of a model is a measure of how far the weights are allowed to 'step' in the direction of the gradient. If the LR is constant and large, (as was the case in our last example) the model takes large steps towards the optimal solution. This is not ideal: as the weights of the model start to converge on the solution, the large steps taken by the optimiser may undershoot it. A constant, small LR is not ideal either: the tiny steps taken by the optimiser mean that the model would take a very long time to converge, or learn. Therefore, we want a combination of these two: relatively large LR early on in the training (to quickly get in the neighbourhood of the optimal solution) and a progressively smaller LR as we get closer and closer to the solution (so that we don't undershoot it).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d758a",
   "metadata": {},
   "source": [
    "### 5.1.1 ExponentialLR\n",
    "In Pytorch, this is handled by a group of classes called learning rate schedulers. To demonstrate how they work, we will be using the `ExponentialLR` scheduler, which applies the following function to reduce the learning rate every epoch:    \n",
    "\n",
    "$l_{epoch} = g * l_{epoch - 1}$ \n",
    "\n",
    "where \n",
    "* $l_{epoch}$ is the new learning rate to be set for the next epoch,\n",
    "* $g$ is the hyperparameter gamma, which controls how quickly the learning rate decays, and\n",
    "* $l_{epoch - 1}$ is the learning rate of the current epoch\n",
    "\n",
    "_Note: The initial LR chosen for this exercise was actually relatively high and is only intended to exaggerate the difference an adaptive LR scheduler makes. Real applications initialise their LRs to something many orders of magnitude smaller_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "# Exponential LR visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1b275",
   "metadata": {},
   "source": [
    "### 5.1.2 Modifying the training loop \n",
    "Let's apply what we've learned about adaptive LRs to our training loop. We update our `train_model` function to accept a new parameter `lr_scheduler`, which is an instance of the `ExponentialLR` class. You can of course use any LR scheduler in the `torch.optim.lr_scheduler` module, and some may be better suited to a particular task than others. `ExponentialLR` was chosen here for its simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_gpu_lr(model, epochs, train_dl, optimiser, lr_scheduler):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            \n",
    "            image_batch = image_batch.to(DEVICE)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            \n",
    "            output = model(image_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()  \n",
    "            \n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            correct += int(torch.eq(preds, label_batch).sum())\n",
    "            total += batch_sz\n",
    "            minibatch_accuracy = 100 * correct / total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            \n",
    "        # Call the LR scheduler every epoch so that it can update the learning rate used by the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data split from  DATA_PATH variable\n",
    "train_dl, val_dl, test_dl = ...\n",
    "# instantiate simple network\n",
    "network = ...\n",
    "# instantiate SGD optimiser\n",
    "optim = ...\n",
    "# instantiate ExponentialLR learning rate scheduler\n",
    "lr_sch = ...\n",
    "# move network to DEVICE\n",
    "...\n",
    "# call train function\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd68c4d",
   "metadata": {},
   "source": [
    "### 5.1.3 Debrief: Simple model with adaptive LR\n",
    "The model should now be performing at above 90% accuracy. Getting closer, but not quite there yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a245ee",
   "metadata": {},
   "source": [
    "## 5.2 The convolutional network\n",
    "Back to improving model performance - we're currently hovering around 90%, so let's modify our simple linear network so that the input is a convolutional layer. This is more suited to image data than a simple fully connected layer like we had in the previous few examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_conv_net():\n",
    "    pass\n",
    "\n",
    "# Print the model's summary (you can check week 4's lesson for a reminder\n",
    "# on how to do this)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf187f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the model now expects a 3-d input (channels * width * height), we need to modify our training function:\n",
    "def train_model_gpu_lr_conv(model, epochs, train_dl, optimiser, lr_scheduler):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            image_batch = image_batch.to(DEVICE) # 1 channel, 28 * 28 pixels\n",
    "            output = model(image_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()  \n",
    "            \n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            correct += int(torch.eq(preds, label_batch).sum())\n",
    "            total += batch_sz\n",
    "            minibatch_accuracy = 100 * correct / total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            \n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data split from DATA_PATH variable\n",
    "train_dl, val_dl, test_dl = ...\n",
    "# instantiate the simple convolutional network\n",
    "network = ...\n",
    "# instantiate SGD optimiser\n",
    "optim = ...\n",
    "# instantiate exponential learning rate scheduler\n",
    "lr_sch = ...\n",
    "# move model to DEVICE\n",
    "network = ...\n",
    "# call latest training function\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74f978",
   "metadata": {},
   "source": [
    "### 5.2.1 Debrief: Simple Conv net \n",
    "You should now see a significantly higher accuracy (around 98 - 99%). However, training accuracy is only half the story. We know now that the model can predict with very high accuracy what digit it's looking at as long as it's already seen it before. But what if it has to make a prediction on a number it's not seen before?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931037d5",
   "metadata": {},
   "source": [
    "## 5.3 The validation epoch\n",
    "This is the reason we split out dataset into train, validation and test splits earlier. At the end of every train epoch, we additionally run a validation epoch to assure ourselves that the model is actually learning a decent representation of the dataset in general, and not over-fitting on the training data. Once again, we must make the necessary changes to our training function (getting to be a bit of a mouthful now :D). The `train_dl` argument has been replaced by a dictionary that holds all the dataloaders associated with our dataset, so that we can reference them by name in the function body without passing a lot of arguments to it. It is generally a good idea to wrap closely related arguments in some sort of structure like a dictionary - leads to less cluttered function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e21d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = ...\n",
    "\n",
    "def train_model_gpu_lr_conv_valid(model, epochs, dataloaders, optimiser, lr_scheduler):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):        \n",
    "        #######################TRAINING STEP###################################\n",
    "        model.train()  # set model to training mode \n",
    "        # select train dataloader\n",
    "                \n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            image_batch = image_batch.to(DEVICE).reshape(batch_sz, 1, 28, 28) \n",
    "            output = model(image_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            preds_train = torch.argmax(output, dim=1)\n",
    "            correct_train += int(torch.eq(preds_train, label_batch).sum())\n",
    "            total_train += batch_sz\n",
    "            minibatch_accuracy_train = 100 * correct_train / total_train\n",
    "            \n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps_train}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy_train:.5f}, LR: {lr_scheduler.get_last_lr()[0]:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "        lr_scheduler.step()\n",
    "        ########################################################################\n",
    "        print(\"\") # Create newline between progress bars\n",
    "        #######################VALIDATION STEP##################################\n",
    "        ########################################################################\n",
    "        print(\"\")  # Create newline between progress bars\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate simple conv net\n",
    "network = ...\n",
    "# instantiate SGD optimiser\n",
    "optim = ...\n",
    "# instantiate exponential LR scheduler\n",
    "lr_sch = ...\n",
    "# move model to DEVICE\n",
    "network = ...\n",
    "# call latest training function\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b4c01",
   "metadata": {},
   "source": [
    "### 5.3.1 Debrief: Simple Conv net with validation \n",
    "You will notice that the evaluation accuracy lags behind the training accuracy ever so slightly. This is normal, and to be expected: it shows the model is not quite as good at predicting unseen numbers as it is at predicting ones it's seen before, although it's pretty close! The gap between the train and val metrics is called the generlisation gap and is a measure of how well the model performs on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
