{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0498a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"./saved_models\"\n",
    "DATASET_PREFIX = \"./data/MNIST\"\n",
    "DATA_PATH = f\"{DATASET_PREFIX}/raw/mnist_dataset.csv\"\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # The collate function is used to tell the Pytorch DataLoader how to handle datapoints from the MNISTDataset we \n",
    "    # defined earlier and pack them into a batch. By default (i.e. no specific collate_fn is passed), the DataLoader\n",
    "    # would simply add the dataset items to an array and ensure that the array is of a certain size (the batch size).\n",
    "    # This would normally not be a problem if we were working with text data that is of a fixed length. However, \n",
    "    # in our case, we are working with image data, and our dataset (so far) does not actually contain the images \n",
    "    # themselves, but filepaths to them, along with labels. For this reason, we must define a custom collate function\n",
    "    # that reads these images and their labels into memory, and returns them side-by-side so we can use them in our \n",
    "    # neural network.\n",
    "    def load_image_tensor(filepath):\n",
    "        # This funtion is only visible inside custom_collate_fn and does the work of loading a single image into\n",
    "        # a Pytorch Tensor\n",
    "        img = Image.open(filepath)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.PILToTensor()\n",
    "        ])\n",
    "        img_tensor = transform(img)\n",
    "        return img_tensor\n",
    "\n",
    "    image_batch_tensor = torch.FloatTensor(len(batch), 28, 28) # We define a tensor of the same size as our image batch to store loaded images into\n",
    "    image_tensors = []\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        image_tensor = load_image_tensor(f\"{DATASET_PREFIX}/{item[0]}\") # load a single image\n",
    "        image_tensors.append(image_tensor) # put image into a list \n",
    "        labels.append(item[1]) # put the same image's label into another list\n",
    "\n",
    "\n",
    "    torch.cat(image_tensors, out=image_batch_tensor) # torch.cat simply concatenates a list of individual tensors (image_tensors) into a single Pytorch tensor (image_batch_tensor)\n",
    "    label_batch_tensor = torch.FloatTensor(labels).type(torch.int64) # use the label list to create a torch tensor of ints\n",
    "    return (image_batch_tensor, label_batch_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_path, batch_sz=100, train_val_test_split=[0.3, 0.2, 0.5]):\n",
    "    # This is a convenience funtion that returns dataset splits of train, val and test according to the fractions specified in the arguments\n",
    "    assert sum(train_val_test_split) == 1, \"Train, val and test fractions should sum to 1!\"  # Always a good idea to use static asserts when processing arguments that are passed in by a user!\n",
    "    train_dataset = MNISTDataset(data_path)  # Instantiating our previously defined dataset\n",
    "    \n",
    "    # This code generates the actual number of items that goes into each split using the user-supplied fractions\n",
    "    train_val_split = list(\n",
    "        map( # map applies a given function to each element of a list\n",
    "            lambda frac: round(frac * len(train_dataset)), # anonymous function that multiplies the fraction by total length of dataset and rounds to the nearest integer\n",
    "            train_val_test_split # the list to apply the function to\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # split dataset into train, val and test\n",
    "    train_split, val_split, test_split = random_split(train_dataset, train_val_split)\n",
    "    \n",
    "    # Use Pytorch DataLoader to load each split into memory. It's important to pass in our custom collate function, so it knows how to interpret the \n",
    "    # data and load it. num_workers tells the DataLoader how many CPU threads to use so that data can be loaded in parallel, which is faster\n",
    "    n_cpus = mp.cpu_count() # returns number of CPU cores on this machine\n",
    "    train_dl = DataLoader(train_split, \n",
    "                          batch_size=batch_sz, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=custom_collate_fn,\n",
    "                          num_workers=n_cpus)            \n",
    "    val_dl = DataLoader(val_split, \n",
    "                        batch_size=batch_sz, \n",
    "                        shuffle=True, \n",
    "                        collate_fn=custom_collate_fn,\n",
    "                        num_workers=n_cpus)\n",
    "    test_dl = DataLoader(test_split,\n",
    "                         batch_size=batch_sz,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=custom_collate_fn,\n",
    "                         num_workers=n_cpus)\n",
    "    return train_dl, val_dl, test_dl\n",
    "\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, filepath: str): \n",
    "        super().__init__()\n",
    "        self.dataframe = pd.read_csv(filepath) # Load data from CSV filepath defined earlier into a Pandas dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) # Return size of our dataframe\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.dataframe.iloc[i] # Return the `i`th item in our dataframe\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Visualisation helper functions. When working with image data, it can be helpful to define such functions to\n",
    "# make sure that the data visually \"looks right\". It is also a pretty good indication that you probably got all\n",
    "# the dataloading code correct!\n",
    "\n",
    "def image_grid(batch, ncols=4):\n",
    "    height, width = batch[0].shape\n",
    "    nrows = len(batch)//ncols # calculate the number of rows based on the number of columns needed by the user\n",
    "    \n",
    "    img_grid = (batch.reshape(nrows, ncols, height, width)\n",
    "              .swapaxes(1,2)\n",
    "              .reshape(height*nrows, width*ncols))\n",
    "    \n",
    "    return img_grid\n",
    "\n",
    "\n",
    "def show_batch(batch, title=\"Image batch\", cols=4):\n",
    "    N = len(batch)\n",
    "    if N > cols:\n",
    "        assert N % cols == 0, \"Number of cols must be a multiple of N\"\n",
    "    \n",
    "    result = image_grid(batch)\n",
    "    fig = plt.figure(figsize=(5., 5.))\n",
    "    plt.suptitle(f\"{title} [{int(N/cols)}x{cols}]\")\n",
    "    plt.imshow(result)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
