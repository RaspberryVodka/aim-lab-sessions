{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe29295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kavi/.virtualenvs/aim-lab-sessions/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Get all the variables, classes and functions we defined in the previous lessons\n",
    "from vars.week_3 import *\n",
    "\n",
    "# Import new modules\n",
    "import torch.nn as nn \n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a2395",
   "metadata": {},
   "source": [
    "# 4. Defining simple models \n",
    "## 4.1 nn.Sequential\n",
    "The `Sequential` class is very simple: it accepts a sequence of neural network `Modules` as arguments and arranges them such that the output of one is automatically sent to the input of the next in line. This saves us a bit of time writing some code, but has some drawbacks, as we shall see shortly. The following is the simplest possible neural network. It consists only of an input layer, 1 hidden layer and an output layer. It is good practice to print out the summary of your network using `torchsummary.summary`. This lets you inspect your networks parameters and the input/output sizes of each layer. Interestingly, it also acts as a sort of sanity checker for your model, because it will complain if the input/output sizes of your layers aren't compatible with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0334b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 128]         100,480\n",
      "              ReLU-3                  [-1, 128]               0\n",
      "            Linear-4                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_simple_linear_net():\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),                # Input is a 2d array of pixel values, so we flatten it out to 1d\n",
    "        nn.Linear(28*28, 128),       # Input layer connects each input node to each hidden node. MNIST images are 28*28 pixels, hidden size can be anything we want\n",
    "        nn.ReLU(),                   # ReLU activation only lets a signal through if it is > 0\n",
    "        nn.Linear(128, 10)  # Output connects each node in the hidden layer to 10 output classes - the number of digits we want to classify!\n",
    "        \n",
    "    )\n",
    "\n",
    "summary(get_simple_linear_net(), input_size=(1, 28, 28), device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368199e5",
   "metadata": {},
   "source": [
    "### 4.1.1  Simple training loop\n",
    "Now that we've defined a network, we can start training it! Let's define the simplest possible training function, for which we only require the model, number of training epochs, the dataloader and an optimisation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f5ef147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, train_dl, optimiser):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()  # set model to training mode\n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            # Prepare data and label batches \n",
    "            batch_sz = len(image_batch)\n",
    "            output = model(image_batch)            \n",
    "            # explain categorical cross-entropy loss\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "            # Zero gradients and backpropagate the losses\n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()  # update model weights based on loss gradients\n",
    "\n",
    "            # Update the total number of correct predictions and calculate accuracy\n",
    "            # this needs some explanation, draw diagram\n",
    "            preds = torch.argmax(output, dim=1) \n",
    "            correct += int(torch.eq(preds, label_batch).sum())\n",
    "            total += batch_sz\n",
    "            minibatch_accuracy = 100 * correct / total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec4a08cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NllLossBackward0' object has no attribute 'grad_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m network \u001b[38;5;241m=\u001b[39m get_simple_linear_net()                    \u001b[38;5;66;03m# Creating an instance of our network\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optim \u001b[38;5;241m=\u001b[39m SGD(network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)  \u001b[38;5;66;03m# Stochastic gradient descent optimiser\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, train_dl, optimiser)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# explain categorical cross-entropy loss\u001b[39;00m\n\u001b[1;32m     14\u001b[0m losses \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(output, label_batch)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mdir\u001b[39m(\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_fn\u001b[49m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Zero gradients and backpropagate the losses\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NllLossBackward0' object has no attribute 'grad_fn'"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# Defining network hyperparameters\n",
    "epochs = 5\n",
    "batch_sz = 32\n",
    "learning_rate = 0.005\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)   # Creating a data split\n",
    "\n",
    "network = get_simple_linear_net()                    # Creating an instance of our network\n",
    "optim = SGD(network.parameters(), lr=learning_rate)  # Stochastic gradient descent optimiser\n",
    "train_model(network, epochs, train_dl, optim)        # Calling our training function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc68cc9",
   "metadata": {},
   "source": [
    "### 4.1.2 Debrief: Simple model with simple training loop\n",
    "At the end of the training loop, our model performs pretty well - should be around 80-90% accuracy most of the time. This is definitely better than random chance, so our model seems to have learned something about the dataset and can make good predictions. But it could be better! Before we look into improving this, there is something else that needs fixing... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d7284",
   "metadata": {},
   "source": [
    "### 4.1.3 Training device\n",
    "Something you may have noticed so far is that the training loop runs quite slowly. 5 epochs is not a very long time at all in the machine learning world and it still takes a while to complete. This because we've been asking the CPU to do all the tensor calculations needed to update the weights. This is a bad idea because GPUs are much more efficient at processing large amounts of data in parallel. You should always use a GPU to train machine learning models if one is available. Pytorch makes it very easy to detect GPU availability and transfer code you've written for a CPU to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # CUDA is a low-level toolkit that provides libraries for interacting with NVIDIA graphics cards\n",
    "print(f\"Using device: {DEVICE}\") # This should print out `Using device: cuda` if Pytorch detects a GPU on your system \n",
    "\n",
    "\n",
    "# We also need to modify our training loop to accept the device as an argument, and transfer input tensors to the GPU device\n",
    "def train_model_gpu(model, epochs, train_dl, optimiser):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            \n",
    "            # Transferring image and label tensors to GPU #\n",
    "            image_batch = image_batch.to(DEVICE)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            ###############################################\n",
    "            \n",
    "            output = model(image_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "                        \n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()  \n",
    "            \n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            correct += int(torch.eq(preds, label_batch).sum())\n",
    "            total += batch_sz\n",
    "            minibatch_accuracy = 100 * correct / total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635bbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we need to transfer our model to the device as well, and can begin training\n",
    "network = get_simple_linear_net()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "network = network.to(DEVICE)\n",
    "train_model_gpu(network, epochs, train_dl, optim)\n",
    "\n",
    "# You should see a speedup in training speed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
